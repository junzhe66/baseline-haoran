{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5786c867",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\".\")\n",
    "\n",
    "# also disable grad to save memory\n",
    "import torch\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "DEVICE = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "import yaml\n",
    "import torch\n",
    "from omegaconf import OmegaConf\n",
    "from taming.models.vqgan import VQModel, GumbelVQ\n",
    "\n",
    "def load_config(config_path, display=False):\n",
    "    config = OmegaConf.load(config_path)\n",
    "    if display:\n",
    "        print(yaml.dump(OmegaConf.to_container(config)))\n",
    "    return config\n",
    "\n",
    "def load_vqgan(config, ckpt_path=None, is_gumbel=False):\n",
    "    if is_gumbel:\n",
    "        model = GumbelVQ(**config.model.params)\n",
    "    else:\n",
    "        model = VQModel(**config.model.params)\n",
    "    if ckpt_path is not None:\n",
    "        sd = torch.load(ckpt_path, map_location=\"cpu\")[\"state_dict\"]\n",
    "        missing, unexpected = model.load_state_dict(sd, strict=False)\n",
    "    return model.eval()\n",
    "\n",
    "def reconstruct_with_vqgan(x, model):\n",
    "    # could also use model(x) for reconstruction but use explicit encoding and decoding here\n",
    "    z, _, [_, _, indices], _ = model.encode(x)\n",
    "    #print(f\"VQGAN --- {model.__class__.__name__}: latent shape: {z.shape[2:]}\")\n",
    "    xrec = model.decode(z)\n",
    "    return xrec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f459a037",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with z of shape (1, 256, 16, 16) = 65536 dimensions.\n",
      "loaded pretrained LPIPS loss from taming/modules/autoencoder/lpips/vgg.pth\n",
      "VQLPIPSWithDiscriminator running with hinge loss.\n"
     ]
    }
   ],
   "source": [
    "config1024 = load_config(\"logs/vqgan_imagenet_f16_1024/configs/model.yaml\", display=False)\n",
    "#config16384 = load_config(\"logs/vqgan_imagenet_f16_16384/configs/model.yaml\", display=True)\n",
    "\n",
    "model1024 = load_vqgan(config1024, ckpt_path=\"logs/vqgan_imagenet_f16_1024/checkpoints/last.ckpt\").to(DEVICE)\n",
    "#model16384 = load_vqgan(config16384, ckpt_path=\"logs/vqgan_imagenet_f16_16384/checkpoints/last.ckpt\").to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2aaa76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os, sys\n",
    "import requests\n",
    "import PIL\n",
    "from PIL import Image\n",
    "from PIL import ImageDraw, ImageFont\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21543509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset\n",
    "import torch\n",
    "import sys\n",
    "from nuwa_pytorch import VQGanVAE\n",
    "import h5py\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "def eventGeneration(start_time, obs_time = 3 ,lead_time = 6, time_interval = 30):\n",
    "    # Generate event based on starting time point, return a list: [[t-4,...,t-1,t], [t+1,...,t+72]]\n",
    "    # Get the start year, month, day, hour, minute\n",
    "    year = int(start_time[0:4])\n",
    "    month = int(start_time[4:6])\n",
    "    day = int(start_time[6:8])\n",
    "    hour = int(start_time[8:10])\n",
    "    minute = int(start_time[10:12])\n",
    "    #print(datetime(year=year, month=month, day=day, hour=hour, minute=minute))\n",
    "    times = [(datetime(year, month, day, hour, minute) + timedelta(minutes=time_interval * (x+1))) for x in range(lead_time)]\n",
    "    lead = [dt.strftime('%Y%m%d%H%M') for dt in times]\n",
    "    times = [(datetime(year, month, day, hour, minute) - timedelta(minutes=time_interval * x)) for x in range(obs_time)]\n",
    "    obs = [dt.strftime('%Y%m%d%H%M') for dt in times]\n",
    "    obs.reverse()\n",
    "    return lead, obs\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import h5py\n",
    "import numpy as np\n",
    "from torchvision.transforms import ToTensor, Compose, CenterCrop\n",
    "class radarDataset(Dataset):\n",
    "    def __init__(self, root_dir, event_times, obs_number = 3, pred_number = 6, transform=None):\n",
    "        # event_times is an array of starting time t(string)\n",
    "        # transform is the preprocessing functions\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.event_times = event_times\n",
    "        self.obs_number = obs_number\n",
    "        self.pred_number = pred_number\n",
    "    def __len__(self):\n",
    "        return len(self.event_times)\n",
    "    def __getitem__(self, idx):\n",
    "        start_time = str(self.event_times[idx])\n",
    "        time_list_pre, time_list_obs = eventGeneration(start_time, self.obs_number, self.pred_number)\n",
    "        output = []\n",
    "        time_list = time_list_obs + time_list_pre\n",
    "        #print(time_list)\n",
    "        for time in time_list:\n",
    "            year = time[0:4]\n",
    "            month = time[4:6]\n",
    "            #path = self.root_dir + year + '/' + month + '/' + 'RAD_NL25_RAC_MFBS_EM_5min_' + time + '_NL.h5'\n",
    "            path = self.root_dir + year + '/' + month + '/' + 'RAD_NL25_RAP_5min_' + time + '.h5'\n",
    "            image = np.array(h5py.File(path)['image1']['image_data'])\n",
    "            #image = np.ma.masked_where(image == 65535, image)\n",
    "\n",
    "            image = image[264:520,242:498]\n",
    "            image[image == 65535] = 0\n",
    "            image = image.astype('float32')\n",
    "            image = image/100*12\n",
    "            image = np.clip(image, 0, 128)\n",
    "            image = image/40\n",
    "            output.append(image)\n",
    "        output = torch.permute(torch.tensor(np.array(output)), (1, 2, 0))\n",
    "        output = self.transform(np.array(output))\n",
    "        return output, start_time\n",
    "#root_dir = '/users/hbi/data/RAD_NL25_RAC_MFBS_EM_5min/'\n",
    "#dataset = radarDataset(root_dir, [\"200808031600\"], transform = Compose([ToTensor(),CenterCrop(256)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5cf4c47a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extreme: 1535 823 627\n",
      "Full Dataset: 32183 13352 14170\n",
      "7873 7575 7557\n",
      "6986\n"
     ]
    }
   ],
   "source": [
    "# develop dataset\n",
    "from torch.cuda.amp import autocast\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import pandas as pd\n",
    "root_dir = '/home/hbi/RAD_NL25_RAP_5min/' \n",
    "\n",
    "# M-L, top 5%\n",
    "df_train = pd.read_csv('/users/hbi/taming-transformers/training_Delfland08-14.csv', header = None)\n",
    "event_times = df_train[0].to_list()\n",
    "dataset_train = radarDataset(root_dir, event_times, transform = Compose([ToTensor()]))           \n",
    "\n",
    "df_test = pd.read_csv('/users/hbi/taming-transformers/testing_Delfland18-20.csv', header = None)\n",
    "event_times = df_test[0].to_list()\n",
    "dataset_test = radarDataset(root_dir, event_times, transform = Compose([ToTensor()]))\n",
    "\n",
    "df_vali = pd.read_csv('/users/hbi/taming-transformers/validation_Delfland15-17.csv', header = None)\n",
    "event_times = df_vali[0].to_list()\n",
    "dataset_vali = radarDataset(root_dir, event_times, transform = Compose([ToTensor()]))\n",
    "\n",
    "df_train_aa = pd.read_csv('/users/hbi/taming-transformers/training_Aa08-14.csv', header = None)\n",
    "event_times = df_train_aa[0].to_list()\n",
    "dataset_train_aa = radarDataset(root_dir, event_times, transform = Compose([ToTensor()]))    \n",
    "\n",
    "df_train_dw = pd.read_csv('/users/hbi/taming-transformers/training_Dwar08-14.csv', header = None)\n",
    "event_times = df_train_dw[0].to_list()\n",
    "dataset_train_dw = radarDataset(root_dir, event_times, transform = Compose([ToTensor()]))   \n",
    "\n",
    "df_train_re = pd.read_csv('/users/hbi/taming-transformers/training_Regge08-14.csv', header = None)\n",
    "event_times = df_train_re[0].to_list()\n",
    "dataset_train_re = radarDataset(root_dir, event_times, transform = Compose([ToTensor()]))    \n",
    "\n",
    "# S-M, top 20%\n",
    "df_train_full = pd.read_csv('/users/hbi/taming-transformers/training_Delfland08-14_20.csv', header = None)\n",
    "event_times = df_train_full[0].to_list()\n",
    "dataset_train_full = radarDataset(root_dir, event_times, transform = Compose([ToTensor()]))           \n",
    "\n",
    "df_test_full = pd.read_csv('/users/hbi/taming-transformers/testing_Delfland18-20_20.csv', header = None)\n",
    "event_times = df_test_full[0].to_list()\n",
    "dataset_test_full = radarDataset(root_dir, event_times, transform = Compose([ToTensor()]))\n",
    "\n",
    "df_vali_full = pd.read_csv('/users/hbi/taming-transformers/validation_Delfland15-17_20.csv', header = None)\n",
    "event_times = df_vali_full[0].to_list()\n",
    "dataset_vali_full = radarDataset(root_dir, event_times, transform = Compose([ToTensor()]))\n",
    "\n",
    "# extreme, top 1%\n",
    "df_train_ext = pd.read_csv('/users/hbi/taming-transformers/training_Delfland08-14_ext.csv', header = None)\n",
    "event_times = df_train_ext[0].to_list()\n",
    "dataset_train_ext = radarDataset(root_dir, event_times, transform = Compose([ToTensor()]))           \n",
    "\n",
    "df_test_ext = pd.read_csv('/users/hbi/taming-transformers/testing_Delfland18-20_ext.csv', header = None)\n",
    "event_times = df_test_ext[0].to_list()\n",
    "dataset_test_ext = radarDataset(root_dir, event_times, transform = Compose([ToTensor()]))\n",
    "\n",
    "df_vali_ext = pd.read_csv('/users/hbi/taming-transformers/validation_Delfland15-17_ext.csv', header = None)\n",
    "event_times = df_vali_ext[0].to_list()\n",
    "dataset_vali_ext = radarDataset(root_dir, event_times, transform = Compose([ToTensor()]))\n",
    "\n",
    "print(\"Extreme:\", len(dataset_train_ext), len(dataset_test_ext), len(dataset_vali_ext))\n",
    "print(\"Full Dataset:\", len(dataset_train_full), len(dataset_test_full), len(dataset_vali_full))\n",
    "loaders = { 'train_de5' :DataLoader(dataset_train, batch_size=1, shuffle=False, num_workers=0),\n",
    "            'test' :DataLoader(dataset_test, batch_size=1, shuffle=True, num_workers=8), \n",
    "           'valid' :DataLoader(dataset_vali, batch_size=1, shuffle=False, num_workers=0),\n",
    "           'train_full' :DataLoader(dataset_train_full, batch_size=1, shuffle=False, num_workers=0),\n",
    "            'test_full' :DataLoader(dataset_test_full, batch_size=1, shuffle=False, num_workers=0), \n",
    "           'valid_full' :DataLoader(dataset_vali_full, batch_size=1, shuffle=False, num_workers=0),\n",
    "          'train_ext' :DataLoader(dataset_train_ext, batch_size=2, shuffle=True, num_workers=0),\n",
    "            'test_ext' :DataLoader(dataset_test_ext, batch_size=2, shuffle=True, num_workers=0), \n",
    "           'valid_ext' :DataLoader(dataset_vali_ext, batch_size=2, shuffle=True, num_workers=0), \n",
    "           'train_aa5' :DataLoader(dataset_train_aa, batch_size=1, shuffle=False, num_workers=0),\n",
    "           'train_dw5' :DataLoader(dataset_train_dw, batch_size=1, shuffle=False, num_workers=0),\n",
    "           'train_re5' :DataLoader(dataset_train_re, batch_size=1, shuffle=False, num_workers=0)}\n",
    "print(len(loaders['train_de5']),len(loaders['train_aa5']),len(loaders['train_dw5']))\n",
    "print(len(loaders['test'])*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e8779e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = torch.load('/users/hbi/taming-transformers/vae1024_256km_epoch2', map_location=\"cpu\")\n",
    "model1024.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7fa12f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from nuwa_pytorch.nuwa_pytorch import Sparse3DNA, Attention, SparseCross2DNA, cast_tuple, mult_reduce, calc_same_padding, unfoldNd, FeedForward, ShiftVideoTokens, SandwichNorm, StableLayerNorm\n",
    "from nuwa_pytorch.nuwa_pytorch import AxialPositionalEmbedding, default, padding_to_multiple_of, exists, MList, einsum\n",
    "from einops import rearrange\n",
    "from nuwa_pytorch.vqgan_vae import stable_softmax\n",
    "class SparseCross3DNA(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        video_shape,\n",
    "        kernel_size = 3,\n",
    "        dilation = 1,\n",
    "        heads = 8,\n",
    "        dim_head = 64,\n",
    "        dropout = 0.,\n",
    "        causal = False,\n",
    "        query_num_frames_chunk = None,\n",
    "        rel_pos_bias = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head * heads\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n",
    "        self.to_kv = nn.Linear(dim, inner_dim * 2, bias = False)\n",
    "        self.talking_heads = nn.Conv2d(heads, heads, 1, bias = False)\n",
    "        self.to_out = nn.Linear(inner_dim, dim)\n",
    "        self.dilation = cast_tuple(dilation, size = 3)\n",
    "        self.kernel_size = cast_tuple(kernel_size, size = 3)\n",
    "        self.kernel_numel = mult_reduce(self.kernel_size)\n",
    "       # relative positional bias per head, if needed\n",
    "        self.rel_pos_bias = AxialPositionalEmbedding(heads, shape = self.kernel_size) if rel_pos_bias else None\n",
    "        # calculate padding\n",
    "        self.padding_frame = calc_same_padding(self.kernel_size[0], self.dilation[0])\n",
    "        self.padding_height = calc_same_padding(self.kernel_size[1], self.dilation[1])\n",
    "        self.padding_width = calc_same_padding(self.kernel_size[2], self.dilation[2])\n",
    "        self.video_padding = (self.padding_width, self.padding_width, self.padding_height, self.padding_height, self.padding_frame, self.padding_frame)\n",
    "        # save video shape and calculate max number of tokens\n",
    "        self.video_shape = video_shape\n",
    "        max_frames, fmap_size, _ = video_shape\n",
    "        max_num_tokens = torch.empty(video_shape).numel()\n",
    "        self.max_num_tokens = max_num_tokens\n",
    "        # how many query tokens to process at once to limit peak memory usage, by multiple of frame tokens (fmap_size ** 2)\n",
    "        self.query_num_frames_chunk = default(query_num_frames_chunk, max_frames)\n",
    "        # precalculate causal mask\n",
    "        indices = torch.arange(max_num_tokens)\n",
    "        shaped_indices = rearrange(indices, '(f h w) -> 1 1 f h w', f = max_frames, h = fmap_size, w = fmap_size)\n",
    "        padded_indices = F.pad(shaped_indices, self.video_padding, value = max_num_tokens) # padding has value of max tokens so to be masked out\n",
    "        unfolded_indices = unfoldNd(padded_indices, kernel_size = self.kernel_size, dilation = self.dilation)\n",
    "        unfolded_indices = rearrange(unfolded_indices, '1 k n -> n k')\n",
    "        # if causal, compare query and key indices and make sure past cannot see future\n",
    "        # if not causal, just mask out the padding\n",
    "\n",
    "        if causal:\n",
    "            mask = rearrange(indices, 'n -> n 1') < unfolded_indices\n",
    "        else:\n",
    "            mask = unfolded_indices == max_num_tokens\n",
    "\n",
    "        #mask = F.pad(mask, (1, 0), value = False) # bos tokens never get masked out\n",
    "        self.register_buffer('mask', mask)\n",
    "        #print(self.mask.shape)\n",
    "\n",
    "    def forward(self, x, context, **kwargs):\n",
    "        b, n, _, h, device = *x.shape, self.heads, x.device\n",
    "        n_context = context.shape[1]\n",
    "        # more variables\n",
    "\n",
    "        dilation = self.dilation\n",
    "        kernel_size = self.kernel_size\n",
    "        video_padding = self.video_padding\n",
    "        fmap_size = self.video_shape[1]\n",
    "        tokens_per_frame = fmap_size ** 2\n",
    "\n",
    "        padding = padding_to_multiple_of(n - 1, tokens_per_frame)\n",
    "        num_frames = (n + padding) // tokens_per_frame\n",
    "        num_frames_input = (n_context + padding) // tokens_per_frame\n",
    "        \n",
    "        # derive queries / keys / values\n",
    "\n",
    "        q, k, v = (self.to_q(x), *self.to_kv(context).chunk(2, dim = -1))\n",
    "        \n",
    "        bos_only = n == 1\n",
    "        if bos_only:\n",
    "            return self.to_out(v)\n",
    "        # split out heads\n",
    "\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> (b h) n d', h = h), (q, k, v))\n",
    "        \n",
    "       \n",
    "        # take care of bos\n",
    "\n",
    "        #q = q[:, 1:]\n",
    "        #bos_value = q[:, :1]\n",
    "        \n",
    "        # scale queries\n",
    "        q = q * self.scale\n",
    "\n",
    "        # reshape keys and values to video and add appropriate padding along all dimensions (frames, height, width)\n",
    "        k, v = map(lambda t: rearrange(t, 'b (f h w) d -> b d f h w',  h = fmap_size, w = fmap_size), (k, v))\n",
    "        k, v = map(lambda t: F.pad(t, video_padding), (k, v))\n",
    "        #print(k.shape)\n",
    "        # axial relative pos bias\n",
    "\n",
    "        rel_pos_bias = None\n",
    "\n",
    "        if exists(self.rel_pos_bias):\n",
    "            rel_pos_bias = rearrange(self.rel_pos_bias(), 'j h -> h 1 j')\n",
    "            rel_pos_bias = F.pad(rel_pos_bias, (1, 0), value = 0.)\n",
    "\n",
    "        # put the attention processing code in a function\n",
    "        # to allow for processing queries in chunks of frames\n",
    "\n",
    "        out = []\n",
    "\n",
    "        def attend(q, k, v, mask, kernel_size):\n",
    "            chunk_length = q.shape[1]\n",
    "            k, v = map(lambda t: unfoldNd(t, kernel_size = kernel_size, dilation = dilation), (k, v))\n",
    "            k, v = map(lambda t: rearrange(t, 'b (d j) i -> b i j d', j = self.kernel_numel), (k, v))\n",
    "            k, v = map(lambda t: t[:, :chunk_length], (k, v))\n",
    "\n",
    "            # calculate sim\n",
    "\n",
    "            sim = einsum('b i d, b i j d -> b i j', q, k)\n",
    "            #print(q.shape, k.shape, sim.shape)\n",
    "            # add rel pos bias, if needed\n",
    "\n",
    "            if exists(rel_pos_bias):\n",
    "                sim = sim + rel_pos_bias\n",
    "\n",
    "            # causal mask\n",
    "\n",
    "            \n",
    "            if exists(mask):\n",
    "                mask_value = -torch.finfo(sim.dtype).max               \n",
    "                mask = rearrange(mask, 'i j -> 1 i j')\n",
    "                sim = sim.masked_fill(mask, mask_value)\n",
    "            \n",
    "            # attention\n",
    "\n",
    "            attn = stable_softmax(sim, dim = -1)\n",
    "\n",
    "            attn = rearrange(attn, '(b h) ... -> b h ...', h = h)\n",
    "            attn = self.talking_heads(attn)\n",
    "            attn = rearrange(attn, 'b h ... -> (b h) ...')\n",
    "\n",
    "            attn = self.dropout(attn)\n",
    "\n",
    "            # aggregate values\n",
    "\n",
    "            return einsum('b i j, b i j d -> b i d', attn, v)\n",
    "\n",
    "        # process queries in chunks\n",
    "\n",
    "        frames_per_chunk = num_frames_input\n",
    "        chunk_size = frames_per_chunk * tokens_per_frame\n",
    "        q_chunks = q.split(chunk_size, dim = 1)\n",
    "        mask = self.mask\n",
    "        #print(q.shape, chunk_size)\n",
    "        for ind, q_chunk in enumerate(q_chunks):\n",
    "            #print(ind, q_chunks[ind].shape, mask.shape)\n",
    "            q_chunk = q_chunks[ind]\n",
    "            length = q_chunk.shape[1]\n",
    "            q_chunk = F.pad(input=q_chunk, pad=(0, 0, 0, chunk_size-length), mode='constant', value=float(\"-Inf\"))\n",
    "            mask_chunk = mask\n",
    "\n",
    "            # slice the keys and values to the appropriate frames, accounting for padding along frames dimension\n",
    "\n",
    "            k_slice, v_slice = k,v\n",
    "            # calculate output chunk\n",
    "            out_chunk = attend(\n",
    "                q = q_chunk,\n",
    "                k = k_slice,\n",
    "                v = v_slice,\n",
    "                mask = mask_chunk,\n",
    "                kernel_size = kernel_size,\n",
    "            )\n",
    "\n",
    "            out_chunk = out_chunk[:,:length,:]\n",
    "            out.append(out_chunk)\n",
    "            \n",
    "        # combine all chunks\n",
    "        out = torch.cat(out, dim = 1)\n",
    "        # append bos value\n",
    "\n",
    "        #out = torch.cat((bos_value, out), dim = 1)  # bos will always adopt its own value, since it pays attention only to itself\n",
    "        # merge heads\n",
    "\n",
    "        out = rearrange(out, '(b h) n d -> b n (h d)', h = h)\n",
    "        return self.to_out(out)\n",
    "    \n",
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        dim,\n",
    "        depth,\n",
    "        causal = False,\n",
    "        heads = 8,\n",
    "        dim_head = 64,\n",
    "        ff_mult = 4,\n",
    "        cross_attend = False,\n",
    "        attn_dropout = 0.,\n",
    "        ff_dropout = 0.,\n",
    "        ff_chunk_size = None,\n",
    "        cross_2dna_attn = False,\n",
    "        cross_2dna_image_size = None,\n",
    "        cross_2dna_kernel_size = 3,\n",
    "        cross_2dna_dilations = (1,),\n",
    "        cross_3dna_attn = False,\n",
    "        cross_3dna_image_size = None,\n",
    "        cross_3dna_kernel_size = 3,\n",
    "        cross_3dna_dilations = (1,),\n",
    "        sparse_3dna_attn = False,\n",
    "        sparse_3dna_kernel_size = 3,\n",
    "        sparse_3dna_video_shape = None,\n",
    "        sparse_3dna_query_num_frames_chunk = None,\n",
    "        sparse_3dna_dilations = (1,),\n",
    "        sparse_3dna_rel_pos_bias = False,\n",
    "        shift_video_tokens = False,\n",
    "        rotary_pos_emb = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = MList([])\n",
    "\n",
    "        for ind in range(depth):\n",
    "            if sparse_3dna_attn:\n",
    "                dilation = sparse_3dna_dilations[ind % len(sparse_3dna_dilations)]\n",
    "\n",
    "                self_attn = Sparse3DNA(\n",
    "                    dim = dim,\n",
    "                    heads = heads,\n",
    "                    dim_head = dim_head,\n",
    "                    causal = causal,\n",
    "                    kernel_size = sparse_3dna_kernel_size,\n",
    "                    dilation = dilation,\n",
    "                    video_shape = sparse_3dna_video_shape,\n",
    "                    query_num_frames_chunk = sparse_3dna_query_num_frames_chunk,\n",
    "                    rel_pos_bias = sparse_3dna_rel_pos_bias,\n",
    "                )\n",
    "            else:\n",
    "                self_attn = Attention(\n",
    "                    dim = dim,\n",
    "                    heads = heads,\n",
    "                    dim_head = dim_head,\n",
    "                    causal = causal,\n",
    "                    dropout = attn_dropout\n",
    "                )\n",
    "\n",
    "            cross_attn = None\n",
    "            \n",
    "            if cross_attend:\n",
    "                if cross_2dna_attn:\n",
    "                    dilation = cross_2dna_dilations[ind % len(cross_2dna_dilations)]\n",
    "\n",
    "                    cross_attn = SparseCross2DNA(\n",
    "                        dim = dim,\n",
    "                        heads = heads,\n",
    "                        dim_head = dim_head,\n",
    "                        dropout = attn_dropout,\n",
    "                        image_size = cross_2dna_image_size,\n",
    "                        kernel_size = cross_2dna_kernel_size,\n",
    "                        dilation = dilation\n",
    "                    )\n",
    "                \n",
    "                elif cross_3dna_attn:\n",
    "                    \n",
    "                    cross_attn = SparseCross3DNA(\n",
    "                        dim = dim,\n",
    "                        heads = heads,\n",
    "                        dim_head = dim_head,\n",
    "                        dropout = attn_dropout,\n",
    "                        video_shape = cross_3dna_image_size,\n",
    "                        kernel_size = cross_3dna_kernel_size,\n",
    "                    )\n",
    "                \n",
    "                else:\n",
    "                    cross_attn = Attention(\n",
    "                        dim = dim,\n",
    "                        heads = heads,\n",
    "                        dim_head = dim_head,\n",
    "                        dropout = attn_dropout\n",
    "                    )\n",
    "\n",
    "            ff = FeedForward(dim = dim, mult = ff_mult, dropout = ff_dropout, chunk_size = ff_chunk_size)\n",
    "\n",
    "            if sparse_3dna_attn and shift_video_tokens:\n",
    "                fmap_size = sparse_3dna_video_shape[-1]\n",
    "                self_attn = ShiftVideoTokens(self_attn, image_size = fmap_size)\n",
    "                ff        = ShiftVideoTokens(ff, image_size = fmap_size)\n",
    "\n",
    "            self.layers.append(MList([\n",
    "                SandwichNorm(dim = dim, fn = self_attn),\n",
    "                SandwichNorm(dim = dim, fn = cross_attn) if cross_attend else None,\n",
    "                SandwichNorm(dim = dim, fn = ff)\n",
    "            ]))\n",
    "\n",
    "        self.norm = StableLayerNorm(dim)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x,\n",
    "        mask = None,\n",
    "        context = None,\n",
    "        context_mask = None\n",
    "    ):\n",
    "        for attn, cross_attn, ff in self.layers:\n",
    "            x = attn(x, mask = mask) + x\n",
    "\n",
    "            if exists(cross_attn):\n",
    "                x = cross_attn(x, context = context, mask = mask, context_mask = context_mask) + x\n",
    "\n",
    "            x = ff(x) + x\n",
    "\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c75aa3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer for second stage\n",
    "# transformer for second stage\n",
    "from torch import nn\n",
    "import nuwa_pytorch.nuwa_pytorch\n",
    "from nuwa_pytorch.nuwa_pytorch import MList, Embedding, AxialPositionalEmbedding, ReversibleTransformer, default, eval_decorator, prob_mask_like, exists\n",
    "from nuwa_pytorch.nuwa_pytorch import padding_to_multiple_of, einsum\n",
    "from nuwa_pytorch.vqgan_vae import stable_softmax\n",
    "from einops import rearrange, repeat\n",
    "import torch.nn.functional as F\n",
    "from main import instantiate_from_config\n",
    "#from dice_loss import DiceLoss\n",
    "class nuwa_v2v(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        dim,\n",
    "        vae = None,\n",
    "        image_size = 256,\n",
    "        codebook_size = 1024,\n",
    "        compression_rate = 16,\n",
    "        video_out_seq_len = 5,\n",
    "        video_in_seq_len = 5,\n",
    "        video_dec_depth = 6,\n",
    "        video_dec_dim_head = 64,\n",
    "        video_dec_heads = 8,\n",
    "        attn_dropout = 0.,\n",
    "        ff_dropout = 0.,\n",
    "        ff_chunk_size = None,\n",
    "        embed_gradient_frac = 0.2,\n",
    "        shift_video_tokens = True,\n",
    "        sparse_3dna_kernel_size = 3,\n",
    "        sparse_3dna_query_num_frames_chunk = None,\n",
    "        sparse_3dna_dilation = 1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # VAE\n",
    "        #self.first_stage_model = vae.copy_for_eval()\n",
    "        self.image_size = image_size\n",
    "        num_image_tokens = codebook_size\n",
    "        fmap_size = image_size // compression_rate\n",
    "        self.video_fmap_size = fmap_size\n",
    "        self.video_frames_in = video_in_seq_len\n",
    "        self.video_frames_out = video_out_seq_len\n",
    "        self.image_embedding = Embedding(num_image_tokens, dim, frac_gradient=embed_gradient_frac)\n",
    "        # cycle dilation for sparse 3d-nearby attention \n",
    "        sparse_3dna_dilations = tuple(range(1, sparse_3dna_dilation + 1)) if not isinstance(sparse_3dna_dilation, (list, tuple)) else sparse_3dna_dilation  # ???\n",
    "        \n",
    "        video_shape_in = (video_in_seq_len, fmap_size, fmap_size)\n",
    "        video_shape_out = (video_out_seq_len, fmap_size, fmap_size)\n",
    "        self.video_in_pos_emb = AxialPositionalEmbedding(dim, shape = video_shape_in)\n",
    "        self.video_out_pos_emb = AxialPositionalEmbedding(dim, shape = video_shape_out)\n",
    "        self.video_bos = nn.Parameter(torch.randn(dim))\n",
    "        \n",
    "        # 3DNA Encoder\n",
    "        video_shape_input = (video_in_seq_len, fmap_size, fmap_size)\n",
    "    \n",
    "        # 3DNA Decoder\n",
    "        #self.video_bos = nn.Parameter(torch.randn(dim))#\n",
    "        #self.image_embedding = Embedding(num_image_tokens, dim, frac_gradient = embed_gradient_frac)#???\n",
    "        # self.video_pos_emb = AxialPositionalEmbedding(dim, shape = video_shape)#???\n",
    "\n",
    "        video_shape_output = (video_out_seq_len, fmap_size, fmap_size)\n",
    "        self.video_decode_transformer = Transformer(\n",
    "            dim = dim,\n",
    "            depth = video_dec_depth,\n",
    "            heads = video_dec_heads,\n",
    "            dim_head = video_dec_dim_head,\n",
    "            causal = True,\n",
    "            cross_attend = True,\n",
    "            attn_dropout = attn_dropout,\n",
    "            ff_dropout = ff_dropout,\n",
    "            ff_chunk_size = ff_chunk_size,\n",
    "            cross_3dna_attn = True,\n",
    "            cross_3dna_image_size = video_shape_input ,\n",
    "            cross_3dna_kernel_size = (5,3,3),\n",
    "            shift_video_tokens = shift_video_tokens,\n",
    "            sparse_3dna_video_shape = video_shape_output,\n",
    "            sparse_3dna_attn = True,\n",
    "            sparse_3dna_kernel_size = sparse_3dna_kernel_size,\n",
    "            sparse_3dna_dilations = sparse_3dna_dilations,\n",
    "            sparse_3dna_query_num_frames_chunk = sparse_3dna_query_num_frames_chunk\n",
    "        )\n",
    "\n",
    "        self.to_logits = nn.Linear(dim, num_image_tokens)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    @eval_decorator\n",
    "    def generate(\n",
    "        self,\n",
    "        *,\n",
    "        video_input_latent,\n",
    "        video_output_latent = None,\n",
    "        filter_thres = 0.9,\n",
    "        temperature = 1.,\n",
    "        num_frames = 6,\n",
    "        prior_weight = 1,\n",
    "        alpha = 1\n",
    "    ):\n",
    "        #video_indice_last_frame = video_input_latent[0,:,512:768]\n",
    "        device = video_input_latent.device\n",
    "        batch = video_input_latent.shape[0]\n",
    "        \n",
    "        frame_indices_input = video_input_latent.squeeze(1)\n",
    "        frame_embeddings_input = self.image_embedding(frame_indices_input)\n",
    "        frame_embeddings_input = self.video_in_pos_emb().repeat(batch,1,1) + frame_embeddings_input\n",
    "        \n",
    "        bos = repeat(self.video_bos, 'd -> b 1 d', b = batch)\n",
    "        video_indices = torch.empty((batch, 0), device = device, dtype = torch.long)\n",
    "        num_tokens_per_frame = self.video_fmap_size ** 2\n",
    "        \n",
    "        num_frames = default(num_frames, self.video_frames_out)\n",
    "        total_video_tokens =  num_tokens_per_frame * num_frames\n",
    "        \n",
    "        pos_emb = self.video_out_pos_emb()\n",
    "        \n",
    "        prior = frame_indices_input[:, 512:].repeat(1,num_frames)\n",
    "        \n",
    "        for ind in range(total_video_tokens):\n",
    "            print(ind, '/', total_video_tokens, end = '\\r')\n",
    "            \n",
    "            #if video_output_latent != None and ind<256:\n",
    "            #    #print(video_indice_last_frame.shape, video_indice_last_frame[:,ind])\n",
    "            #    video_indices = torch.cat((video_indices, video_output_latent[:,:,ind]), dim = 1)\n",
    "            #    continue\n",
    "            \n",
    "            video_indices_input = video_indices\n",
    "            num_video_tokens = video_indices.shape[1]\n",
    "            #video_indices_input = video_output_latent[:,: num_video_tokens]\n",
    "            \n",
    "            frame_embeddings = self.image_embedding(video_indices_input)\n",
    "            frame_embeddings = pos_emb[:frame_embeddings.shape[1]] + frame_embeddings\n",
    "            frame_embeddings = torch.cat((bos, frame_embeddings), dim = 1)\n",
    "            \n",
    "            frame_embeddings = self.video_decode_transformer(\n",
    "                frame_embeddings,\n",
    "                context = frame_embeddings_input\n",
    "            )\n",
    "        \n",
    "            logits = self.to_logits(frame_embeddings)\n",
    "            logits = logits[:, -1, :]\n",
    "            logits[:,int(prior[:,ind])] *= prior_weight\n",
    "            \n",
    "            #w = torch.load('weight_post.pt').to(DEVICE) \n",
    "            #logits[logits<0] = 0\n",
    "            #filtered_logits = top_k(logits, thres = filter_thres)\n",
    "            #filtered_logits = filtered_logits*(w.unsqueeze(0))\n",
    "            \n",
    "            filtered_logits = top_k(logits, thres = filter_thres)\n",
    "            probs = F.softmax(filtered_logits, dim=-1)\n",
    "            sample = torch.multinomial(probs, 1).squeeze(-1)\n",
    "            \n",
    "            #sample = gumbel_sample(filtered_logits, temperature = temperature, dim = -1)\n",
    "            sample = rearrange(sample, 'b -> b 1')\n",
    "            video_indices = torch.cat((video_indices, sample), dim = 1)\n",
    "            \n",
    "            if (ind+1) % num_tokens_per_frame == 0:\n",
    "                n =  int((ind+1) / num_tokens_per_frame)\n",
    "                prior = video_indices[:, (n-1)*int(num_tokens_per_frame):].repeat(1,num_frames)\n",
    "                prior_weight = max(prior_weight*alpha, 1)\n",
    "        \n",
    "        return video_indices\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        *,\n",
    "        #video_input, # raw observation video here\n",
    "        #video_output, # raw prediction video here\n",
    "        video_input_latent,\n",
    "        video_output_latent,\n",
    "        return_loss = False,\n",
    "        cond_dropout_prob = 0\n",
    "    ):\n",
    "        device = video_input_latent.device\n",
    "        batch = video_input_latent.shape[0]\n",
    "        seq_out_len = self.video_frames_out\n",
    "        seq_in_len = self.video_frames_in\n",
    "        \n",
    "        #get indices\n",
    "        frame_indices_input = video_input_latent.squeeze(1)\n",
    "        frame_indices_output = video_output_latent.squeeze(1)\n",
    "        frame_indices_output = frame_indices_output[:, :-1] if return_loss else frame_indices_output\n",
    "        \n",
    "        #indices to embedding\n",
    "        frame_embeddings_input = self.image_embedding(frame_indices_input)\n",
    "        frame_embeddings_prediction = self.image_embedding(frame_indices_output)\n",
    "\n",
    "        #position encoding\n",
    "        frame_embeddings_input = self.video_in_pos_emb().repeat(batch,1,1) + frame_embeddings_input\n",
    "        if return_loss:\n",
    "            frame_embeddings_prediction = self.video_out_pos_emb()[:-1].repeat(batch,1,1) + frame_embeddings_prediction\n",
    "            # shift right\n",
    "            bos = repeat(self.video_bos, 'd -> b 1 d', b = batch)\n",
    "            frame_embeddings_prediction = torch.cat((bos, frame_embeddings_prediction), dim = 1)\n",
    "        else:\n",
    "            frame_embeddings_prediction = self.video_out_pos_emb().repeat(batch,1,1) + frame_embeddings_prediction\n",
    "        \n",
    "        #transformer \n",
    "        frame_embeddings_prediction = self.video_decode_transformer(\n",
    "            frame_embeddings_prediction,\n",
    "            context = frame_embeddings_input\n",
    "        )\n",
    "        #print(frame_embeddings_prediction.shape)\n",
    "        logits = self.to_logits(frame_embeddings_prediction)\n",
    "        \n",
    "        #print(t2-t1, t3-t2, t4-t3, t5-t4)\n",
    "        if not return_loss:\n",
    "            return logits\n",
    "        #weight = torch.load('weight_clip_10.pt').to(device)\n",
    "        loss = F.cross_entropy(rearrange(logits, 'b n c -> b c n'), video_output_latent.squeeze(1))\n",
    "        \n",
    "        #dice_loss = DiceLoss(smooth = 1, with_logits = True, ohem_ratio = 0, \n",
    "        #                     alpha = 0.01, reduction = \"mean\",  index_label_position=True,  square_denominator = True)\n",
    "        #print(rearrange(logits, 'b n c -> (b n) c').shape, rearrange(video_output_latent.squeeze(1), 'b n -> (b n)').shape)\n",
    "        #loss = dice_loss(rearrange(logits, 'b n c -> (b n) c'), rearrange(video_output_latent.squeeze(1), 'b n -> (b n)'))\n",
    "        \n",
    "        return loss, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1b77d57",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "#torch.cuda.empty_cache()\n",
    "nuwa = nuwa_v2v(dim=512 , vae=None,\n",
    "                image_size = 256,\n",
    "                codebook_size = 1024,\n",
    "                compression_rate = 16,\n",
    "                video_out_seq_len = 6,\n",
    "                video_in_seq_len = 3,\n",
    "                video_dec_depth = 12,\n",
    "                video_dec_dim_head = 64,\n",
    "                video_dec_heads = 4,\n",
    "                sparse_3dna_kernel_size = (7,3,3),\n",
    "                attn_dropout = 0,\n",
    "                ff_dropout = 0).to(DEVICE)\n",
    "\n",
    "for layer in nuwa.video_decode_transformer.layers[:12]:\n",
    "    for param in layer.parameters():\n",
    "        param.requires_grad = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0efa866b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pysteps configuration file found at: /users/junzheyin/anaconda3/envs/myenv/lib/python3.8/site-packages/pysteps/pystepsrc\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pysteps.verification.detcatscores import det_cat_fct\n",
    "from pysteps.verification.detcontscores import det_cont_fct\n",
    "from pysteps.verification.spatialscores import intensity_scale\n",
    "from pysteps.visualization import plot_precip_field\n",
    "from nuwa_pytorch.nuwa_pytorch import top_k, gumbel_sample\n",
    "import random\n",
    "\n",
    "device = DEVICE\n",
    "def decode_to_img(index, num_frame = 6):\n",
    "        model1024.to(device)\n",
    "        b = index.shape[0]\n",
    "        #index = self.permuter(index, reverse=True)\n",
    "        for t in range(num_frame):\n",
    "            index_frame = index[:,t*nuwa.video_fmap_size*nuwa.video_fmap_size:(t+1)*nuwa.video_fmap_size*nuwa.video_fmap_size]\n",
    "            bhwc = (b,nuwa.video_fmap_size,nuwa.video_fmap_size,3)\n",
    "            quant_z = model1024.quantize.get_codebook_entry(\n",
    "                index_frame.reshape(-1), shape=bhwc)\n",
    "            quant_z = rearrange(quant_z, '(b h w) d -> b d h w',b = b, h = nuwa.video_fmap_size, w = nuwa.video_fmap_size)\n",
    "            #print(quant_z.shape)\n",
    "            if t==0:\n",
    "                x = model1024.decode(quant_z)\n",
    "                x = x.unsqueeze(1)\n",
    "            else:\n",
    "                x = torch.cat((x,model1024.decode(quant_z).unsqueeze(1)),1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1499a9de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nuwa.eval()\n",
    "checkpoint = torch.load( '/bulk/junzheyin/nuwa_newvae_sparse7_evl_75_epoch12',map_location=\"cpu\")\n",
    "nuwa.load_state_dict(checkpoint, strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "25c2a65a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from nuwa_pytorch import VQGanVAE\n",
    "from nuwa_pytorch.optimizer import get_optimizer\n",
    "vae = VQGanVAE(\n",
    "    dim = 256,\n",
    "    channels = 1,               # default is 3, but can be changed to any value for the training of the segmentation masks (sketches)\n",
    "    image_size = 256,           # image size\n",
    "    num_layers = 4,             # number of downsampling layers\n",
    "    num_resnet_blocks = 2,      # number of resnet blocks\n",
    "    vq_codebook_size = 1024,    # codebook size\n",
    "    vq_decay = 0.8 ,             # codebook exponential decay\n",
    "    use_hinge_loss = True,\n",
    "    use_vgg_and_gan = True\n",
    ").to(DEVICE)\n",
    "checkpoint = torch.load('/bulk/junzheyin/aaa/vae_epoch80', map_location = 'cpu')\n",
    "vae.load_state_dict(checkpoint, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "135e9dc8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1535 / 1536\r"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[1, 16, 16, 3]' is invalid for input of size 65536",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/users/junzheyin/taming-transformers/nuwa_date.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcondor.et.tudelft.nl/users/junzheyin/taming-transformers/nuwa_date.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m video_generate1 \u001b[39m=\u001b[39m nuwa\u001b[39m.\u001b[39mgenerate(video_input_latent \u001b[39m=\u001b[39m indice_obs, \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcondor.et.tudelft.nl/users/junzheyin/taming-transformers/nuwa_date.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=35'>36</a>\u001b[0m                                 video_output_latent \u001b[39m=\u001b[39m indice_pre,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcondor.et.tudelft.nl/users/junzheyin/taming-transformers/nuwa_date.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=36'>37</a>\u001b[0m                                 filter_thres \u001b[39m=\u001b[39m \u001b[39m0.98\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcondor.et.tudelft.nl/users/junzheyin/taming-transformers/nuwa_date.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=39'>40</a>\u001b[0m                                 prior_weight \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcondor.et.tudelft.nl/users/junzheyin/taming-transformers/nuwa_date.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=40'>41</a>\u001b[0m                                 alpha \u001b[39m=\u001b[39m \u001b[39m0.8\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcondor.et.tudelft.nl/users/junzheyin/taming-transformers/nuwa_date.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39m#video_generate1 = nuwa(video_input_latent= indice_obs,video_output_latent= indice_pre)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcondor.et.tudelft.nl/users/junzheyin/taming-transformers/nuwa_date.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39m#video_generate1 = torch.argmax(video_generate1, dim = 2)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bcondor.et.tudelft.nl/users/junzheyin/taming-transformers/nuwa_date.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=43'>44</a>\u001b[0m video_predict1 \u001b[39m=\u001b[39m decode_to_img(video_generate1, num_frame \u001b[39m=\u001b[39;49m n)\u001b[39m.\u001b[39msqueeze(\u001b[39m0\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcondor.et.tudelft.nl/users/junzheyin/taming-transformers/nuwa_date.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=44'>45</a>\u001b[0m video_predict1 \u001b[39m=\u001b[39m vae\u001b[39m.\u001b[39mcodebook_indices_to_video(video_generate1)\u001b[39m.\u001b[39msqueeze(\u001b[39m0\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcondor.et.tudelft.nl/users/junzheyin/taming-transformers/nuwa_date.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n):\n",
      "\u001b[1;32m/users/junzheyin/taming-transformers/nuwa_date.ipynb Cell 13\u001b[0m in \u001b[0;36mdecode_to_img\u001b[0;34m(index, num_frame)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcondor.et.tudelft.nl/users/junzheyin/taming-transformers/nuwa_date.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m index_frame \u001b[39m=\u001b[39m index[:,t\u001b[39m*\u001b[39mnuwa\u001b[39m.\u001b[39mvideo_fmap_size\u001b[39m*\u001b[39mnuwa\u001b[39m.\u001b[39mvideo_fmap_size:(t\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m*\u001b[39mnuwa\u001b[39m.\u001b[39mvideo_fmap_size\u001b[39m*\u001b[39mnuwa\u001b[39m.\u001b[39mvideo_fmap_size]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcondor.et.tudelft.nl/users/junzheyin/taming-transformers/nuwa_date.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m bhwc \u001b[39m=\u001b[39m (b,nuwa\u001b[39m.\u001b[39mvideo_fmap_size,nuwa\u001b[39m.\u001b[39mvideo_fmap_size,\u001b[39m3\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bcondor.et.tudelft.nl/users/junzheyin/taming-transformers/nuwa_date.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m quant_z \u001b[39m=\u001b[39m model1024\u001b[39m.\u001b[39;49mquantize\u001b[39m.\u001b[39;49mget_codebook_entry(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcondor.et.tudelft.nl/users/junzheyin/taming-transformers/nuwa_date.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m     index_frame\u001b[39m.\u001b[39;49mreshape(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m), shape\u001b[39m=\u001b[39;49mbhwc)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcondor.et.tudelft.nl/users/junzheyin/taming-transformers/nuwa_date.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m quant_z \u001b[39m=\u001b[39m rearrange(quant_z, \u001b[39m'\u001b[39m\u001b[39m(b h w) d -> b d h w\u001b[39m\u001b[39m'\u001b[39m,b \u001b[39m=\u001b[39m b, h \u001b[39m=\u001b[39m nuwa\u001b[39m.\u001b[39mvideo_fmap_size, w \u001b[39m=\u001b[39m nuwa\u001b[39m.\u001b[39mvideo_fmap_size)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcondor.et.tudelft.nl/users/junzheyin/taming-transformers/nuwa_date.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m#print(quant_z.shape)\u001b[39;00m\n",
      "File \u001b[0;32m~/taming-transformers/taming/modules/vqvae/quantize.py:325\u001b[0m, in \u001b[0;36mVectorQuantizer2.get_codebook_entry\u001b[0;34m(self, indices, shape)\u001b[0m\n\u001b[1;32m    322\u001b[0m z_q \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding(indices)\n\u001b[1;32m    324\u001b[0m \u001b[39mif\u001b[39;00m shape \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 325\u001b[0m     z_q \u001b[39m=\u001b[39m z_q\u001b[39m.\u001b[39;49mview(shape)\n\u001b[1;32m    326\u001b[0m     \u001b[39m# reshape back to match original input shape\u001b[39;00m\n\u001b[1;32m    327\u001b[0m     z_q \u001b[39m=\u001b[39m z_q\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m, \u001b[39m3\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mcontiguous()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[1, 16, 16, 3]' is invalid for input of size 65536"
     ]
    }
   ],
   "source": [
    "#torch.cuda.empty_cache()\n",
    "from collections import Counter\n",
    "import time\n",
    "from pysteps.verification.detcatscores import det_cat_fct\n",
    "from pysteps.verification.detcontscores import det_cont_fct\n",
    "from pysteps.verification.spatialscores import intensity_scale\n",
    "from pysteps.visualization import plot_precip_field\n",
    "from tqdm import tqdm\n",
    "pcc_average = 0\n",
    "counter = Counter()\n",
    "counter2 = Counter()\n",
    "import random\n",
    "index = 0\n",
    "time_list = ['201808101910','202002091900','201906120945','201801022335','201808101805']\n",
    "time_list = ['201808101910']\n",
    "for k in range(1):\n",
    "    for i, (images,time) in enumerate(loaders['test']):\n",
    "        #print(\"{}/{}\".format(i, len(loaders['test'])), end='\\r')\n",
    "        #if i<index:continue\n",
    "        #if i>=index+80:break\n",
    "        if time[0] not in time_list:continue\n",
    "        image1 = images[0].unsqueeze(1)\n",
    "        #a1 = Variable(image1.repeat(1,3,1,1)).to(DEVICE)   # batch x\n",
    "        #a_r1 = reconstruct_with_vqgan(a1, model1024.to(DEVICE))\n",
    "        #quant1, emb_loss1, info1, h1 = model1024.encode(a1)\n",
    "        a1 = Variable(image1.repeat(1, 3, 1, 1)).to(DEVICE)\n",
    "        quant1, emb_loss1, info1 = model1024.encode(a1)\n",
    "        #indice1 = info1[2].view(-1, 16, 16)\n",
    "        indice_obs = info1[2][:768].unsqueeze(0)\n",
    "        indice_pre = info1[2][768:].unsqueeze(0)\n",
    "        #print(indice_obs.shape, indice_pre.shape)\n",
    "        #print(video_predict1.shape)\n",
    "        # Generate\n",
    "        n = 6\n",
    "        video_generate1 = nuwa.generate(video_input_latent = indice_obs, \n",
    "                                        video_output_latent = indice_pre,\n",
    "                                        filter_thres = 0.98,\n",
    "                                        temperature = 0.5,\n",
    "                                        num_frames = n,\n",
    "                                        prior_weight = 2,\n",
    "                                        alpha = 0.8)\n",
    "        #video_generate1 = nuwa(video_input_latent= indice_obs,video_output_latent= indice_pre)\n",
    "        #video_generate1 = torch.argmax(video_generate1, dim = 2)\n",
    "        video_predict1 = decode_to_img(video_generate1, num_frame = n).squeeze(0)\n",
    "        video_predict1 = vae.codebook_indices_to_video(video_generate1).squeeze(0)\n",
    "        for t in range(n):\n",
    "            a1_display = a1[t+3,0,:,:].to('cpu').detach().numpy()*40\n",
    "            #a_r1_display = a_r1[t+3,0,:,:].to('cpu').detach().numpy()*40\n",
    "            a_p1_display = video_predict1[t,0,:,:].to('cpu').detach().numpy()*40\n",
    "            \n",
    "            #np.save('prediction_vqgan2_{} min'.format(-90+(t+4)*30),a_p1_display)\n",
    "            scores_cont = det_cont_fct(a_p1_display, a1_display, thr=0.1)\n",
    "            if True:\n",
    "                print(\"Start Time:\", time[0], \"Lead Time: {} mins\".format(-90+(t+4)*30), \"PCC:\", np.around(scores_cont['corr_p'],3))\n",
    "                plt.figure(figsize=(12, 4))\n",
    "                plt.subplot(131)\n",
    "                plot_precip_field(a1_display, title=\"Original\")\n",
    "                plt.subplot(132)\n",
    "                #plot_precip_field(a_r1_display, title=\"Reconstruction\")\n",
    "                #plt.subplot(133)\n",
    "                plot_precip_field(a_p1_display, title=\"Prediction\")\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233a33ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
