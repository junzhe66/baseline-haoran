{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7baff35d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "1.11.0\n",
      "11.3\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\".\")\n",
    "# also disable grad to save memory\n",
    "import torch\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "import yaml\n",
    "import torch\n",
    "from omegaconf import OmegaConf\n",
    "from taming.models.vqgan import VQModel, GumbelVQ\n",
    "import io\n",
    "import os, sys\n",
    "import requests\n",
    "import PIL\n",
    "from PIL import Image\n",
    "from PIL import ImageDraw, ImageFont\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "print(DEVICE)\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07cfb38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset\n",
    "import torch\n",
    "import sys\n",
    "from nuwa_pytorch import VQGanVAE\n",
    "import h5py\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "def eventGeneration(start_time, obs_time = 3 ,lead_time = 6, time_interval = 30):\n",
    "    # Generate event based on starting time point, return a list: [[t-4,...,t-1,t], [t+1,...,t+72]]\n",
    "    # Get the start year, month, day, hour, minute\n",
    "    year = int(start_time[0:4])\n",
    "    month = int(start_time[4:6])\n",
    "    day = int(start_time[6:8])\n",
    "    hour = int(start_time[8:10])\n",
    "    minute = int(start_time[10:12])\n",
    "    #print(datetime(year=year, month=month, day=day, hour=hour, minute=minute))\n",
    "    times = [(datetime(year, month, day, hour, minute) + timedelta(minutes=time_interval * (x+1))) for x in range(lead_time)]\n",
    "    lead = [dt.strftime('%Y%m%d%H%M') for dt in times]\n",
    "    times = [(datetime(year, month, day, hour, minute) - timedelta(minutes=time_interval * x)) for x in range(obs_time)]\n",
    "    obs = [dt.strftime('%Y%m%d%H%M') for dt in times]\n",
    "    obs.reverse()\n",
    "    return lead, obs\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import h5py\n",
    "import numpy as np\n",
    "from torchvision.transforms import ToTensor, Compose, CenterCrop\n",
    "class radarDataset(Dataset):\n",
    "    def __init__(self, root_dir, event_times, obs_number = 3, pred_number = 18, transform=None):\n",
    "        # event_times is an array of starting time t(string)\n",
    "        # transform is the preprocessing functions\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.event_times = event_times\n",
    "        self.obs_number = obs_number\n",
    "        self.pred_number = pred_number\n",
    "    def __len__(self):\n",
    "        return len(self.event_times)\n",
    "    def __getitem__(self, idx):\n",
    "        start_time = str(self.event_times[idx])\n",
    "        time_list_pre, time_list_obs = eventGeneration(start_time, self.obs_number, self.pred_number)\n",
    "        output = []\n",
    "        time_list = time_list_obs + time_list_pre\n",
    "        #print(time_list)\n",
    "        for time in time_list:\n",
    "            year = time[0:4]\n",
    "            month = time[4:6]\n",
    "            #path = self.root_dir + year + '/' + month + '/' + 'RAD_NL25_RAC_MFBS_EM_5min_' + time + '_NL.h5'\n",
    "            path = self.root_dir + year + '/' + month + '/' + 'RAD_NL25_RAP_5min_' + time + '.h5'\n",
    "            image = np.array(h5py.File(path)['image1']['image_data'])\n",
    "            #image = np.ma.masked_where(image == 65535, image)\n",
    "            image = image[264:520,242:498]\n",
    "            image[image == 65535] = 0\n",
    "            image = image.astype('float32')\n",
    "            image = image/100*12\n",
    "            image = np.clip(image, 0, 128)\n",
    "            image = image/40\n",
    "            output.append(image)\n",
    "        output = torch.permute(torch.tensor(np.array(output)), (1, 2, 0))\n",
    "        output = self.transform(np.array(output))\n",
    "        return output\n",
    "#root_dir = '/users/hbi/data/RAD_NL25_RAC_MFBS_EM_5min/'\n",
    "#dataset = radarDataset(root_dir, [\"200808031600\"], transform = Compose([ToTensor(),CenterCrop(256)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a396504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32183 3493 3560\n"
     ]
    }
   ],
   "source": [
    "# develop dataset\n",
    "from torch.cuda.amp import autocast\n",
    "from torch.autograd import Variable\n",
    "import pandas as pd\n",
    "root_dir = '/home/hbi/RAD_NL25_RAP_5min/' \n",
    "\n",
    "df_train = pd.read_csv('/users/hbi/taming-transformers/training_Delfland08-14_20.csv', header = None)\n",
    "event_times = df_train[0].to_list()\n",
    "dataset_train = radarDataset(root_dir, event_times, transform = Compose([ToTensor()]))  \n",
    "\n",
    "df_train_s = pd.read_csv('/users/hbi/taming-transformers/training_Delfland08-14.csv', header = None)\n",
    "event_times = df_train_s[0].to_list()\n",
    "dataset_train_del = radarDataset(root_dir, event_times, transform = Compose([ToTensor()]))  \n",
    "\n",
    "df_test = pd.read_csv('/users/hbi/taming-transformers/testing_Delfland18-20.csv', header = None)\n",
    "event_times = df_test[0].to_list()\n",
    "dataset_test = radarDataset(root_dir, event_times, transform = Compose([ToTensor()]))\n",
    "\n",
    "df_vali = pd.read_csv('/users/hbi/taming-transformers/validation_Delfland15-17.csv', header = None)\n",
    "event_times = df_vali[0].to_list()\n",
    "dataset_vali = radarDataset(root_dir, event_times, transform = Compose([ToTensor()]))\n",
    "\n",
    "df_train_aa = pd.read_csv('/users/hbi/taming-transformers/training_Aa08-14.csv', header = None)\n",
    "event_times = df_train_aa[0].to_list()\n",
    "dataset_train_aa = radarDataset(root_dir, event_times, transform = Compose([ToTensor()]))  \n",
    "\n",
    "df_train_dw = pd.read_csv('/users/hbi/taming-transformers/training_Dwar08-14.csv', header = None)\n",
    "event_times = df_train_dw[0].to_list()\n",
    "dataset_train_dw = radarDataset(root_dir, event_times, transform = Compose([ToTensor()]))    \n",
    "\n",
    "df_train_re = pd.read_csv('/users/hbi/taming-transformers/training_Regge08-14.csv', header = None)\n",
    "event_times = df_train_re[0].to_list()\n",
    "dataset_train_re = radarDataset(root_dir, event_times, transform = Compose([ToTensor()]))   \n",
    "\n",
    "print(len(dataset_train), len(dataset_test), len(dataset_vali))\n",
    "loaders = { 'train' :DataLoader(dataset_train, batch_size=1, shuffle=True, num_workers=8),\n",
    "            'test' :DataLoader(dataset_test, batch_size=1, shuffle=True, num_workers=8), \n",
    "           'valid' :DataLoader(dataset_vali, batch_size=1, shuffle=False, num_workers=8),\n",
    "          \n",
    "          'train_aa5' :DataLoader(dataset_train_aa, batch_size=1, shuffle=False, num_workers=8),\n",
    "          'train_dw5' :DataLoader(dataset_train_dw, batch_size=1, shuffle=False, num_workers=8),\n",
    "          'train_del5' :DataLoader(dataset_train_del, batch_size=1, shuffle=False, num_workers=8),\n",
    "          'train_re5' :DataLoader(dataset_train_re, batch_size=1, shuffle=False, num_workers=8),\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e05abba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from nuwa_pytorch.nuwa_pytorch import Sparse3DNA, Attention, SparseCross2DNA, cast_tuple, mult_reduce, calc_same_padding, unfoldNd, FeedForward, ShiftVideoTokens, SandwichNorm, StableLayerNorm\n",
    "class SparseCross3DNA(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        video_shape,\n",
    "        kernel_size = 3,\n",
    "        dilation = 1,\n",
    "        heads = 8,\n",
    "        dim_head = 64,\n",
    "        dropout = 0.,\n",
    "        causal = False,\n",
    "        query_num_frames_chunk = None,\n",
    "        rel_pos_bias = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head * heads\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n",
    "        self.to_kv = nn.Linear(dim, inner_dim * 2, bias = False)\n",
    "        self.talking_heads = nn.Conv2d(heads, heads, 1, bias = False)\n",
    "        self.to_out = nn.Linear(inner_dim, dim)\n",
    "        self.dilation = cast_tuple(dilation, size = 3)\n",
    "        self.kernel_size = cast_tuple(kernel_size, size = 3)\n",
    "        self.kernel_numel = mult_reduce(self.kernel_size)\n",
    "       # relative positional bias per head, if needed\n",
    "        self.rel_pos_bias = AxialPositionalEmbedding(heads, shape = self.kernel_size) if rel_pos_bias else None\n",
    "        # calculate padding\n",
    "        self.padding_frame = calc_same_padding(self.kernel_size[0], self.dilation[0])\n",
    "        self.padding_height = calc_same_padding(self.kernel_size[1], self.dilation[1])\n",
    "        self.padding_width = calc_same_padding(self.kernel_size[2], self.dilation[2])\n",
    "        self.video_padding = (self.padding_width, self.padding_width, self.padding_height, self.padding_height, self.padding_frame, self.padding_frame)\n",
    "        # save video shape and calculate max number of tokens\n",
    "        self.video_shape = video_shape\n",
    "        max_frames, fmap_size, _ = video_shape\n",
    "        max_num_tokens = torch.empty(video_shape).numel()\n",
    "        self.max_num_tokens = max_num_tokens\n",
    "        # how many query tokens to process at once to limit peak memory usage, by multiple of frame tokens (fmap_size ** 2)\n",
    "        self.query_num_frames_chunk = default(query_num_frames_chunk, max_frames)\n",
    "        # precalculate causal mask\n",
    "        indices = torch.arange(max_num_tokens)\n",
    "        shaped_indices = rearrange(indices, '(f h w) -> 1 1 f h w', f = max_frames, h = fmap_size, w = fmap_size)\n",
    "        padded_indices = F.pad(shaped_indices, self.video_padding, value = max_num_tokens) # padding has value of max tokens so to be masked out\n",
    "        unfolded_indices = unfoldNd(padded_indices, kernel_size = self.kernel_size, dilation = self.dilation)\n",
    "        unfolded_indices = rearrange(unfolded_indices, '1 k n -> n k')\n",
    "        # if causal, compare query and key indices and make sure past cannot see future\n",
    "        # if not causal, just mask out the padding\n",
    "\n",
    "        if causal:\n",
    "            mask = rearrange(indices, 'n -> n 1') < unfolded_indices\n",
    "        else:\n",
    "            mask = unfolded_indices == max_num_tokens\n",
    "\n",
    "        #mask = F.pad(mask, (1, 0), value = False) # bos tokens never get masked out\n",
    "        self.register_buffer('mask', mask)\n",
    "\n",
    "    def forward(self, x, context, **kwargs):\n",
    "        b, n, _, h, device = *x.shape, self.heads, x.device\n",
    "        n_context = context.shape[1]\n",
    "        # more variables\n",
    "\n",
    "        dilation = self.dilation\n",
    "        kernel_size = self.kernel_size\n",
    "        video_padding = self.video_padding\n",
    "        fmap_size = self.video_shape[1]\n",
    "        tokens_per_frame = fmap_size ** 2\n",
    "\n",
    "        padding = padding_to_multiple_of(n - 1, tokens_per_frame)\n",
    "        num_frames = (n + padding) // tokens_per_frame\n",
    "        num_frames_input = (n_context + padding) // tokens_per_frame\n",
    "        \n",
    "        # derive queries / keys / values\n",
    "\n",
    "        q, k, v = (self.to_q(x), *self.to_kv(context).chunk(2, dim = -1))\n",
    "\n",
    "        # split out heads\n",
    "\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> (b h) n d', h = h), (q, k, v))\n",
    "        \n",
    "        # take care of bos\n",
    "\n",
    "        #q = q[:, 1:]\n",
    "        #bos_value = q[:, :1]\n",
    "        \n",
    "        # scale queries\n",
    "        q = q * self.scale\n",
    "\n",
    "        # reshape keys and values to video and add appropriate padding along all dimensions (frames, height, width)\n",
    "        k, v = map(lambda t: rearrange(t, 'b (f h w) d -> b d f h w',  h = fmap_size, w = fmap_size), (k, v))\n",
    "        k, v = map(lambda t: F.pad(t, video_padding), (k, v))\n",
    "        #print(k.shape)\n",
    "        # axial relative pos bias\n",
    "\n",
    "        rel_pos_bias = None\n",
    "\n",
    "        if exists(self.rel_pos_bias):\n",
    "            rel_pos_bias = rearrange(self.rel_pos_bias(), 'j h -> h 1 j')\n",
    "            rel_pos_bias = F.pad(rel_pos_bias, (1, 0), value = 0.)\n",
    "\n",
    "        # put the attention processing code in a function\n",
    "        # to allow for processing queries in chunks of frames\n",
    "\n",
    "        out = []\n",
    "\n",
    "        def attend(q, k, v, mask, kernel_size):\n",
    "            chunk_length = q.shape[1]\n",
    "\n",
    "            k, v = map(lambda t: unfoldNd(t, kernel_size = kernel_size, dilation = dilation), (k, v))\n",
    "            k, v = map(lambda t: rearrange(t, 'b (d j) i -> b i j d', j = self.kernel_numel), (k, v))\n",
    "            k, v = map(lambda t: t[:, :chunk_length], (k, v))\n",
    "\n",
    "            # calculate sim\n",
    "\n",
    "            sim = einsum('b i d, b i j d -> b i j', q, k)\n",
    "\n",
    "            # add rel pos bias, if needed\n",
    "\n",
    "            if exists(rel_pos_bias):\n",
    "                sim = sim + rel_pos_bias\n",
    "\n",
    "            # causal mask\n",
    "\n",
    "            if exists(mask):\n",
    "                mask_value = -torch.finfo(sim.dtype).max\n",
    "                mask = rearrange(mask, 'i j -> 1 i j')\n",
    "                sim = sim.masked_fill(mask, mask_value)\n",
    "\n",
    "            # attention\n",
    "\n",
    "            attn = stable_softmax(sim, dim = -1)\n",
    "\n",
    "            attn = rearrange(attn, '(b h) ... -> b h ...', h = h)\n",
    "            attn = self.talking_heads(attn)\n",
    "            attn = rearrange(attn, 'b h ... -> (b h) ...')\n",
    "\n",
    "            attn = self.dropout(attn)\n",
    "\n",
    "            # aggregate values\n",
    "\n",
    "            return einsum('b i j, b i j d -> b i d', attn, v)\n",
    "\n",
    "        # process queries in chunks\n",
    "\n",
    "        frames_per_chunk = min(self.query_num_frames_chunk, num_frames)\n",
    "        chunk_size = frames_per_chunk * tokens_per_frame\n",
    "        q_chunks = q.split(chunk_size, dim = 1)\n",
    "        \n",
    "        mask = self.mask\n",
    "\n",
    "        for ind, q_chunk in enumerate(q_chunks):\n",
    "            #print(ind, q_chunks[ind].shape, mask.shape)\n",
    "            q_chunk = q_chunks[ind]\n",
    "            mask_chunk = mask\n",
    "            #print(q_chunk.shape, mask.shape, k.shape, v.shape)\n",
    "            # slice the keys and values to the appropriate frames, accounting for padding along frames dimension\n",
    "\n",
    "            kv_start_pos = ind * frames_per_chunk\n",
    "            kv_end_pos = kv_start_pos + (ind + frames_per_chunk + self.padding_frame * 2)\n",
    "            kv_frame_range = slice(kv_start_pos, kv_end_pos)\n",
    "\n",
    "            k_slice, v_slice = map(lambda t: t[:, :, kv_frame_range], (k, v))\n",
    "            k_slice, v_slice = k,v\n",
    "            # calculate output chunk\n",
    "            out_chunk = attend(\n",
    "                q = q_chunk,\n",
    "                k = k_slice,\n",
    "                v = v_slice,\n",
    "                mask = mask_chunk,\n",
    "                kernel_size = kernel_size,\n",
    "            )\n",
    "            out.append(out_chunk)\n",
    "\n",
    "        # combine all chunks\n",
    "        out = torch.cat(out, dim = 1)\n",
    "        # append bos value\n",
    "\n",
    "        #out = torch.cat((bos_value, out), dim = 1)  # bos will always adopt its own value, since it pays attention only to itself\n",
    "        # merge heads\n",
    "\n",
    "        out = rearrange(out, '(b h) n d -> b n (h d)', h = h)\n",
    "        return self.to_out(out)\n",
    "    \n",
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        dim,\n",
    "        depth,\n",
    "        causal = False,\n",
    "        heads = 8,\n",
    "        dim_head = 64,\n",
    "        ff_mult = 4,\n",
    "        cross_attend = False,\n",
    "        attn_dropout = 0.,\n",
    "        ff_dropout = 0.,\n",
    "        ff_chunk_size = None,\n",
    "        cross_2dna_attn = False,\n",
    "        cross_2dna_image_size = None,\n",
    "        cross_2dna_kernel_size = 3,\n",
    "        cross_2dna_dilations = (1,),\n",
    "        cross_3dna_attn = False,\n",
    "        cross_3dna_image_size = None,\n",
    "        cross_3dna_kernel_size = 5,\n",
    "        cross_3dna_dilations = (1,),\n",
    "        sparse_3dna_attn = False,\n",
    "        sparse_3dna_kernel_size = 3,\n",
    "        sparse_3dna_video_shape = None,\n",
    "        sparse_3dna_query_num_frames_chunk = None,\n",
    "        sparse_3dna_dilations = (1,),\n",
    "        sparse_3dna_rel_pos_bias = False,\n",
    "        shift_video_tokens = False,\n",
    "        rotary_pos_emb = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = MList([])\n",
    "\n",
    "        for ind in range(depth):\n",
    "            if sparse_3dna_attn:\n",
    "                dilation = sparse_3dna_dilations[ind % len(sparse_3dna_dilations)]\n",
    "\n",
    "                self_attn = Sparse3DNA(\n",
    "                    dim = dim,\n",
    "                    heads = heads,\n",
    "                    dim_head = dim_head,\n",
    "                    causal = causal,\n",
    "                    kernel_size = sparse_3dna_kernel_size,\n",
    "                    dilation = dilation,\n",
    "                    video_shape = sparse_3dna_video_shape,\n",
    "                    query_num_frames_chunk = sparse_3dna_query_num_frames_chunk,\n",
    "                    rel_pos_bias = sparse_3dna_rel_pos_bias,\n",
    "                )\n",
    "            else:\n",
    "                self_attn = Attention(\n",
    "                    dim = dim,\n",
    "                    heads = heads,\n",
    "                    dim_head = dim_head,\n",
    "                    causal = causal,\n",
    "                    dropout = attn_dropout\n",
    "                )\n",
    "\n",
    "            cross_attn = None\n",
    "\n",
    "            if cross_attend:\n",
    "                if cross_2dna_attn:\n",
    "                    dilation = cross_2dna_dilations[ind % len(cross_2dna_dilations)]\n",
    "\n",
    "                    cross_attn = SparseCross2DNA(\n",
    "                        dim = dim,\n",
    "                        heads = heads,\n",
    "                        dim_head = dim_head,\n",
    "                        dropout = attn_dropout,\n",
    "                        image_size = cross_2dna_image_size,\n",
    "                        kernel_size = cross_2dna_kernel_size,\n",
    "                        dilation = dilation\n",
    "                    )\n",
    "                \n",
    "                elif cross_3dna_attn:\n",
    "                    \n",
    "                    cross_attn = SparseCross3DNA(\n",
    "                        dim = dim,\n",
    "                        heads = heads,\n",
    "                        dim_head = dim_head,\n",
    "                        dropout = attn_dropout,\n",
    "                        video_shape = cross_3dna_image_size,\n",
    "                        kernel_size = cross_3dna_kernel_size,\n",
    "                    )\n",
    "                \n",
    "                else:\n",
    "                    cross_attn = Attention(\n",
    "                        dim = dim,\n",
    "                        heads = heads,\n",
    "                        dim_head = dim_head,\n",
    "                        dropout = attn_dropout\n",
    "                    )\n",
    "\n",
    "            ff = FeedForward(dim = dim, mult = ff_mult, dropout = ff_dropout, chunk_size = ff_chunk_size)\n",
    "\n",
    "            if sparse_3dna_attn and shift_video_tokens:\n",
    "                fmap_size = sparse_3dna_video_shape[-1]\n",
    "                self_attn = ShiftVideoTokens(self_attn, image_size = fmap_size)\n",
    "                ff        = ShiftVideoTokens(ff, image_size = fmap_size)\n",
    "\n",
    "            self.layers.append(MList([\n",
    "                SandwichNorm(dim = dim, fn = self_attn),\n",
    "                SandwichNorm(dim = dim, fn = cross_attn) if cross_attend else None,\n",
    "                SandwichNorm(dim = dim, fn = ff)\n",
    "            ]))\n",
    "\n",
    "        self.norm = StableLayerNorm(dim)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x,\n",
    "        mask = None,\n",
    "        context = None,\n",
    "        context_mask = None\n",
    "    ):\n",
    "        for attn, cross_attn, ff in self.layers:\n",
    "            x = attn(x, mask = mask) + x\n",
    "\n",
    "            if exists(cross_attn):\n",
    "                x = cross_attn(x, context = context, mask = mask, context_mask = context_mask) + x\n",
    "\n",
    "            x = ff(x) + x\n",
    "\n",
    "        return self.norm(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28d7f3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evt modules\n",
    "from torch import nn\n",
    "from nuwa_pytorch.nuwa_pytorch import Embedding\n",
    "from einops import rearrange\n",
    "# gt_indicator: \n",
    "# indicator: [n, 1024] logits + [1, 1024] masks \n",
    "# gamma: hyperparameter\n",
    "# beta0: propotion of non-extreme tokens\n",
    "# beta1: propotion of extreme tokens \n",
    "def cal_evt_loss(indicator, gt_indicator, gamma = 1, beta0 = 0.95, beta1 = 0.05):\n",
    "    loss1 = -1 * beta0 * torch.pow((1-indicator/gamma),gamma) * gt_indicator * torch.log(indicator)\n",
    "    loss2 = -1 * beta1 * torch.pow((1-(1-indicator)/gamma),gamma) * (1-gt_indicator) * torch.log(1-indicator)\n",
    "    loss = loss1 + loss2 \n",
    "    return loss\n",
    "# logits b * 1024 * 1536\n",
    "# pre_token b * 1 * 1536\n",
    "# ext_tokens 1*15\n",
    "def evt_loss(logits, gt_tokens, ext_tokens, gamma = 1, beta0 = 0.95, beta1 = 0.05, whole = True):\n",
    "    batch = logits.shape[0]\n",
    "    channel = logits.shape[1]\n",
    "    number = logits.shape[2]\n",
    "    device = logits.device\n",
    "    softmax = nn.Softmax(dim = -1)\n",
    "    loss = 0\n",
    "    count = 0\n",
    "    if not whole:\n",
    "        for batch_id in range(batch):\n",
    "            for n in range(number):\n",
    "                gt = gt_tokens[batch_id,n]\n",
    "                if gt not in ext_tokens: continue\n",
    "                prob = softmax(logits[batch_id,:,n])\n",
    "                #for token in ext_tokens:\n",
    "                #    gt_indicator = int(token == gt)\n",
    "                gt_indicator = 1\n",
    "                indicator = prob[gt]\n",
    "                if indicator > 0:\n",
    "                    loss += cal_evt_loss(indicator, gt_indicator, gamma, beta0, beta1)\n",
    "                    count += 1\n",
    "        if count == 0:\n",
    "            return torch.tensor(0).to(DEVICE)\n",
    "        return loss/count\n",
    "    else:\n",
    "        for batch_id in range(batch):\n",
    "            for n in range(number):\n",
    "                gt = gt_tokens[batch_id,n]\n",
    "                if gt not in ext_tokens: continue\n",
    "                gt_indicator = 1\n",
    "                prob = softmax(logits[batch_id,:,n])\n",
    "                indicator = 0\n",
    "                for token in ext_tokens:\n",
    "                    indicator += prob[token]\n",
    "                loss += cal_evt_loss(indicator, gt_indicator, gamma, beta0, beta1)\n",
    "                count += 1\n",
    "        if count == 0:\n",
    "            return torch.tensor(0).to(DEVICE)\n",
    "        return loss/count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "006c7ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer for second stage\n",
    "# transformer for second stage\n",
    "from torch import nn\n",
    "import nuwa_pytorch.nuwa_pytorch\n",
    "from nuwa_pytorch.nuwa_pytorch import MList, Embedding, AxialPositionalEmbedding, ReversibleTransformer, default, eval_decorator, prob_mask_like, exists\n",
    "from nuwa_pytorch.nuwa_pytorch import padding_to_multiple_of, einsum\n",
    "from nuwa_pytorch.vqgan_vae import stable_softmax\n",
    "from einops import rearrange, repeat\n",
    "import torch.nn.functional as F\n",
    "from main import instantiate_from_config\n",
    "#from dice_loss import DiceLoss\n",
    "class nuwa_v2v(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        dim,\n",
    "        vae = None,\n",
    "        image_size = 256,\n",
    "        codebook_size = 1024,\n",
    "        compression_rate = 16,\n",
    "        video_out_seq_len = 5,\n",
    "        video_in_seq_len = 5,\n",
    "        video_dec_depth = 6,\n",
    "        video_dec_dim_head = 64,\n",
    "        video_dec_heads = 8,\n",
    "        attn_dropout = 0.,\n",
    "        ff_dropout = 0.,\n",
    "        ff_chunk_size = None,\n",
    "        embed_gradient_frac = 0.2,\n",
    "        shift_video_tokens = True,\n",
    "        sparse_3dna_kernel_size = 3,\n",
    "        sparse_3dna_query_num_frames_chunk = None,\n",
    "        sparse_3dna_dilation = 1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # VAE\n",
    "        #self.first_stage_model = vae.copy_for_eval()\n",
    "        self.image_size = image_size\n",
    "        num_image_tokens = codebook_size\n",
    "        fmap_size = image_size // compression_rate\n",
    "        self.video_fmap_size = fmap_size\n",
    "        self.video_frames_in = video_in_seq_len\n",
    "        self.video_frames_out = video_out_seq_len\n",
    "        self.image_embedding = Embedding(num_image_tokens, dim, frac_gradient=embed_gradient_frac)\n",
    "        # cycle dilation for sparse 3d-nearby attention \n",
    "        sparse_3dna_dilations = tuple(range(1, sparse_3dna_dilation + 1)) if not isinstance(sparse_3dna_dilation, (list, tuple)) else sparse_3dna_dilation  # ???\n",
    "        \n",
    "        video_shape_in = (video_in_seq_len, fmap_size, fmap_size)\n",
    "        video_shape_out = (video_out_seq_len, fmap_size, fmap_size)\n",
    "        self.video_in_pos_emb = AxialPositionalEmbedding(dim, shape = video_shape_in)\n",
    "        self.video_out_pos_emb = AxialPositionalEmbedding(dim, shape = video_shape_out)\n",
    "        self.video_bos = nn.Parameter(torch.randn(dim))\n",
    "        \n",
    "        # 3DNA Encoder\n",
    "        video_shape_input = (video_in_seq_len, fmap_size, fmap_size)\n",
    "    \n",
    "        # 3DNA Decoder\n",
    "        #self.video_bos = nn.Parameter(torch.randn(dim))#\n",
    "        #self.image_embedding = Embedding(num_image_tokens, dim, frac_gradient = embed_gradient_frac)#???\n",
    "        # self.video_pos_emb = AxialPositionalEmbedding(dim, shape = video_shape)#???\n",
    "\n",
    "        video_shape_output = (video_out_seq_len, fmap_size, fmap_size)\n",
    "        self.video_decode_transformer = Transformer(\n",
    "            dim = dim,\n",
    "            depth = video_dec_depth,\n",
    "            heads = video_dec_heads,\n",
    "            dim_head = video_dec_dim_head,\n",
    "            causal = True,\n",
    "            cross_attend = True,\n",
    "            attn_dropout = attn_dropout,\n",
    "            ff_dropout = ff_dropout,\n",
    "            ff_chunk_size = ff_chunk_size,\n",
    "            cross_3dna_attn = True,\n",
    "            cross_3dna_image_size = video_shape_input ,\n",
    "            cross_3dna_kernel_size = (5,3,3),\n",
    "            shift_video_tokens = shift_video_tokens,\n",
    "            sparse_3dna_video_shape = video_shape_output,\n",
    "            sparse_3dna_attn = True,\n",
    "            sparse_3dna_kernel_size = sparse_3dna_kernel_size,\n",
    "            sparse_3dna_dilations = sparse_3dna_dilations,\n",
    "            sparse_3dna_query_num_frames_chunk = sparse_3dna_query_num_frames_chunk\n",
    "        )\n",
    "\n",
    "        self.to_logits = nn.Linear(dim, num_image_tokens)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    @eval_decorator\n",
    "    def generate(\n",
    "        self,\n",
    "        *,\n",
    "        video_input_latent,\n",
    "        filter_thres = 0.9,\n",
    "        temperature = 1.,\n",
    "        num_frames = 6\n",
    "    ):\n",
    "        device = video_input_latent.device\n",
    "        batch = video_input_latent.shape[0]\n",
    "        \n",
    "        frame_indices_input = video_input_latent.squeeze(1)\n",
    "        frame_embeddings_input = self.image_embedding(frame_indices_input)\n",
    "        frame_embeddings_input = self.video_in_pos_emb().repeat(batch,1,1) + frame_embeddings_input\n",
    "        \n",
    "        bos = repeat(self.video_bos, 'd -> b 1 d', b = batch)\n",
    "        video_indices = torch.empty((batch, 0), device = device, dtype = torch.long)\n",
    "        num_tokens_per_frame = self.video_fmap_size ** 2\n",
    "        \n",
    "        num_frames = default(num_frames, self.video_frames_out)\n",
    "        total_video_tokens =  num_tokens_per_frame * num_frames\n",
    "\n",
    "        pos_emb = self.video_out_pos_emb()\n",
    "\n",
    "        for ind in range(total_video_tokens):\n",
    "            print(ind, '/', total_video_tokens, end = '\\r')\n",
    "            video_indices_input = video_indices\n",
    "            num_video_tokens = video_indices.shape[1]\n",
    "            frame_embeddings = self.image_embedding(video_indices_input)\n",
    "            frame_embeddings = pos_emb[:frame_embeddings.shape[1]] + frame_embeddings\n",
    "            frame_embeddings = torch.cat((bos, frame_embeddings), dim = 1)\n",
    "\n",
    "            frame_embeddings = self.video_decode_transformer(\n",
    "                frame_embeddings,\n",
    "                context = frame_embeddings_input\n",
    "            )\n",
    "\n",
    "            logits = self.to_logits(frame_embeddings)\n",
    "            logits = logits[:, -1, :]\n",
    "            filtered_logits = top_k(logits, thres = filter_thres)\n",
    "            sample = gumbel_sample(filtered_logits, temperature = temperature, dim = -1)\n",
    "            sample = rearrange(sample, 'b -> b 1')\n",
    "            video_indices = torch.cat((video_indices, sample), dim = 1)\n",
    "\n",
    "        return video_indices\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        *,\n",
    "        #video_input, # raw observation video here\n",
    "        #video_output, # raw prediction video here\n",
    "        video_input_latent,\n",
    "        video_output_latent,\n",
    "        return_loss = False,\n",
    "        cond_dropout_prob = 0,\n",
    "        use_evt_loss = False,\n",
    "        evt_loss_weight = 0.5\n",
    "    ):\n",
    "        device = video_input_latent.device\n",
    "        batch = video_input_latent.shape[0]\n",
    "        seq_out_len = self.video_frames_out\n",
    "        seq_in_len = self.video_frames_in\n",
    "        \n",
    "        #get indices\n",
    "        frame_indices_input = video_input_latent.squeeze(1)\n",
    "        frame_indices_output = video_output_latent.squeeze(1)\n",
    "        frame_indices_output = frame_indices_output[:, :-1] if return_loss else frame_indices_output\n",
    "        \n",
    "        #indices to embedding\n",
    "        frame_embeddings_input = self.image_embedding(frame_indices_input)\n",
    "        frame_embeddings_prediction = self.image_embedding(frame_indices_output)\n",
    "\n",
    "        #position encoding\n",
    "        frame_embeddings_input = self.video_in_pos_emb().repeat(batch,1,1) + frame_embeddings_input\n",
    "        if return_loss:\n",
    "            frame_embeddings_prediction = self.video_out_pos_emb()[:-1].repeat(batch,1,1) + frame_embeddings_prediction\n",
    "            # shift right\n",
    "            bos = repeat(self.video_bos, 'd -> b 1 d', b = batch)\n",
    "            frame_embeddings_prediction = torch.cat((bos, frame_embeddings_prediction), dim = 1)\n",
    "        else:\n",
    "            frame_embeddings_prediction = self.video_out_pos_emb().repeat(batch,1,1) + frame_embeddings_prediction\n",
    "        \n",
    "        #transformer \n",
    "        frame_embeddings_prediction = self.video_decode_transformer(\n",
    "            frame_embeddings_prediction,\n",
    "            context = frame_embeddings_input\n",
    "        )\n",
    "        logits = self.to_logits(frame_embeddings_prediction)\n",
    "        \n",
    "        if not return_loss:\n",
    "            return logits\n",
    "        loss = F.cross_entropy(rearrange(logits, 'b n c -> b c n'), video_output_latent.squeeze(1))\n",
    "        total_loss = loss\n",
    "        evt_loss_term = 0 \n",
    "        if use_evt_loss:\n",
    "            ext_tokens = torch.tensor([452, 303, 171, 653, 891, 603, 499, 160, 216, 109, 290, 797, 956, 607, 816]).to(DEVICE)\n",
    "            evt_loss_term = evt_loss_weight * evt_loss(rearrange(logits, 'b n c -> b c n'),  \n",
    "                                               video_output_latent.squeeze(1), \n",
    "                                               ext_tokens, gamma = 1, beta0 = 0.95, beta1 = 0.05, whole = True)  \n",
    "            if not torch.isnan(evt_loss_term):\n",
    "                total_loss = loss + evt_loss_weight *  evt_loss_term    \n",
    "        \n",
    "        return total_loss, loss, logits, evt_loss_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2216183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7575\n"
     ]
    }
   ],
   "source": [
    "class latentDataset(Dataset):\n",
    "    def __init__(self, root_dir, radarset):\n",
    "        # event_times is an array of starting time t(string)\n",
    "        # transform is the preprocessing functions\n",
    "        self.root_dir = root_dir\n",
    "        self.radarset = radarset\n",
    "    def __len__(self):\n",
    "        if self.radarset: return len(self.radarset)\n",
    "        else: return 30000\n",
    "    def __getitem__(self, idx):\n",
    "        dir_file = self.root_dir + str(idx) + '.pt'\n",
    "        if os.path.exists(dir_file):\n",
    "            video_in_latent, video_out_latent = torch.load(dir_file, map_location='cpu')\n",
    "            return video_in_latent, video_out_latent\n",
    "        else:\n",
    "            print(\"File not found\")\n",
    "            return None\n",
    "\n",
    "latent_valid = latentDataset('/bulk/junzheyin/vali1517_newvae/validset', dataset_vali)\n",
    "latent_test = latentDataset('/bulk/junzheyin/test1820/testset', dataset_test)\n",
    "latent_train_de5  = latentDataset('/bulk/junzheyin/train0814_Delf_newvae/trainset', dataset_train_del)\n",
    "latent_train_aa5  = latentDataset('/bulk/junzheyin/train0814_Aa_newvae/trainset', dataset_train_aa)\n",
    "latent_train_dw5  = latentDataset('/bulk/junzheyin/train0814_Dwar_newvae/trainset', dataset_train_dw)\n",
    "latent_train_re5 = latentDataset('/bulk/junzheyin/train0814_Regge_newvae/trainset', dataset_train_re)\n",
    "latent_list = [latent_train_de5, latent_train_aa5, latent_train_dw5, latent_train_re5]\n",
    "latent_train_aadedwre = torch.utils.data.ConcatDataset(latent_list)\n",
    "\n",
    "loaders_latent = { 'test' :DataLoader(latent_test, batch_size=1, shuffle=True, num_workers=0),\n",
    "                  'train_aadedwre' :DataLoader(latent_train_aadedwre, batch_size=1, shuffle=True, num_workers=0),\n",
    "                  #'train_del5' :DataLoader(latent_train_de5, batch_size=1, shuffle=True, num_workers=0),\n",
    "                  'train_aa5' :DataLoader(latent_train_aa5, batch_size=1, shuffle=True, num_workers=0)}\n",
    "print(len(loaders_latent['train_aa5']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1d5c925",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "from torch import optim\n",
    "#torch.cuda.empty_cache()\n",
    "nuwa = nuwa_v2v(dim=512 , vae=None,\n",
    "                image_size = 256,\n",
    "                codebook_size = 1024,\n",
    "                compression_rate = 16,\n",
    "                video_out_seq_len = 6,\n",
    "                video_in_seq_len = 3,\n",
    "                video_dec_depth = 12,\n",
    "                video_dec_dim_head = 64,\n",
    "                video_dec_heads = 4,\n",
    "                sparse_3dna_kernel_size = (7,3,3),\n",
    "                attn_dropout = 0.2,\n",
    "                ff_dropout = 0.2).to(DEVICE)\n",
    "#optimizer = optim.Adam(nuwa.parameters(),lr = 1e-3, weight_decay = 1e-6)\n",
    "optimizer = optim.AdamW(nuwa.parameters(), lr = 1e-3, weight_decay = 0.1)\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef9d3159",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for nuwa_v2v:\n\tUnexpected key(s) in state_dict: \"video_decode_transformer.layers.0.2.fn.fn.net.0.bias\", \"video_decode_transformer.layers.0.2.fn.fn.net.3.bias\", \"video_decode_transformer.layers.1.2.fn.fn.net.0.bias\", \"video_decode_transformer.layers.1.2.fn.fn.net.3.bias\", \"video_decode_transformer.layers.2.2.fn.fn.net.0.bias\", \"video_decode_transformer.layers.2.2.fn.fn.net.3.bias\", \"video_decode_transformer.layers.3.2.fn.fn.net.0.bias\", \"video_decode_transformer.layers.3.2.fn.fn.net.3.bias\", \"video_decode_transformer.layers.4.2.fn.fn.net.0.bias\", \"video_decode_transformer.layers.4.2.fn.fn.net.3.bias\", \"video_decode_transformer.layers.5.2.fn.fn.net.0.bias\", \"video_decode_transformer.layers.5.2.fn.fn.net.3.bias\", \"video_decode_transformer.layers.6.2.fn.fn.net.0.bias\", \"video_decode_transformer.layers.6.2.fn.fn.net.3.bias\", \"video_decode_transformer.layers.7.2.fn.fn.net.0.bias\", \"video_decode_transformer.layers.7.2.fn.fn.net.3.bias\", \"video_decode_transformer.layers.8.2.fn.fn.net.0.bias\", \"video_decode_transformer.layers.8.2.fn.fn.net.3.bias\", \"video_decode_transformer.layers.9.2.fn.fn.net.0.bias\", \"video_decode_transformer.layers.9.2.fn.fn.net.3.bias\", \"video_decode_transformer.layers.10.2.fn.fn.net.0.bias\", \"video_decode_transformer.layers.10.2.fn.fn.net.3.bias\", \"video_decode_transformer.layers.11.2.fn.fn.net.0.bias\", \"video_decode_transformer.layers.11.2.fn.fn.net.3.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/users/junzheyin/taming-transformers/nuwa_train_newvae-Copy1.ipynb Cell 10\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22636f6e646f722e65742e747564656c66742e6e6c222c2275736572223a226a756e7a686579696e227d/users/junzheyin/taming-transformers/nuwa_train_newvae-Copy1.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m checkpoint \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mload( \u001b[39m'\u001b[39m\u001b[39m/home/hbi/nuwa_newvae_sparse7_evl_75_epoch11\u001b[39m\u001b[39m'\u001b[39m,map_location\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22636f6e646f722e65742e747564656c66742e6e6c222c2275736572223a226a756e7a686579696e227d/users/junzheyin/taming-transformers/nuwa_train_newvae-Copy1.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m nuwa\u001b[39m.\u001b[39;49mload_state_dict(checkpoint, strict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22636f6e646f722e65742e747564656c66742e6e6c222c2275736572223a226a756e7a686579696e227d/users/junzheyin/taming-transformers/nuwa_train_newvae-Copy1.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m checkpoint \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mload( \u001b[39m'\u001b[39m\u001b[39m/home/hbi/nuwa_newvae_sparse7_evl_75_epoch11_optim\u001b[39m\u001b[39m'\u001b[39m,map_location\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22636f6e646f722e65742e747564656c66742e6e6c222c2275736572223a226a756e7a686579696e227d/users/junzheyin/taming-transformers/nuwa_train_newvae-Copy1.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m optimizer\u001b[39m.\u001b[39mload_state_dict(checkpoint)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.8/site-packages/torch/nn/modules/module.py:1497\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1492\u001b[0m         error_msgs\u001b[39m.\u001b[39minsert(\n\u001b[1;32m   1493\u001b[0m             \u001b[39m0\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mMissing key(s) in state_dict: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1494\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(k) \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m missing_keys)))\n\u001b[1;32m   1496\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(error_msgs) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m-> 1497\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mError(s) in loading state_dict for \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1498\u001b[0m                        \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   1499\u001b[0m \u001b[39mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for nuwa_v2v:\n\tUnexpected key(s) in state_dict: \"video_decode_transformer.layers.0.2.fn.fn.net.0.bias\", \"video_decode_transformer.layers.0.2.fn.fn.net.3.bias\", \"video_decode_transformer.layers.1.2.fn.fn.net.0.bias\", \"video_decode_transformer.layers.1.2.fn.fn.net.3.bias\", \"video_decode_transformer.layers.2.2.fn.fn.net.0.bias\", \"video_decode_transformer.layers.2.2.fn.fn.net.3.bias\", \"video_decode_transformer.layers.3.2.fn.fn.net.0.bias\", \"video_decode_transformer.layers.3.2.fn.fn.net.3.bias\", \"video_decode_transformer.layers.4.2.fn.fn.net.0.bias\", \"video_decode_transformer.layers.4.2.fn.fn.net.3.bias\", \"video_decode_transformer.layers.5.2.fn.fn.net.0.bias\", \"video_decode_transformer.layers.5.2.fn.fn.net.3.bias\", \"video_decode_transformer.layers.6.2.fn.fn.net.0.bias\", \"video_decode_transformer.layers.6.2.fn.fn.net.3.bias\", \"video_decode_transformer.layers.7.2.fn.fn.net.0.bias\", \"video_decode_transformer.layers.7.2.fn.fn.net.3.bias\", \"video_decode_transformer.layers.8.2.fn.fn.net.0.bias\", \"video_decode_transformer.layers.8.2.fn.fn.net.3.bias\", \"video_decode_transformer.layers.9.2.fn.fn.net.0.bias\", \"video_decode_transformer.layers.9.2.fn.fn.net.3.bias\", \"video_decode_transformer.layers.10.2.fn.fn.net.0.bias\", \"video_decode_transformer.layers.10.2.fn.fn.net.3.bias\", \"video_decode_transformer.layers.11.2.fn.fn.net.0.bias\", \"video_decode_transformer.layers.11.2.fn.fn.net.3.bias\". "
     ]
    }
   ],
   "source": [
    "checkpoint = torch.load( '/home/hbi/nuwa_newvae_sparse7_evl_75_epoch11',map_location=\"cpu\")\n",
    "nuwa.load_state_dict(checkpoint, strict=True)\n",
    "checkpoint = torch.load( '/home/hbi/nuwa_newvae_sparse7_evl_75_epoch11_optim',map_location=\"cpu\")\n",
    "optimizer.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b609e3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from nuwa_pytorch import VQGanVAE\n",
    "from nuwa_pytorch.optimizer import get_optimizer\n",
    "vae = VQGanVAE(\n",
    "    dim = 256,\n",
    "    channels = 1,               # default is 3, but can be changed to any value for the training of the segmentation masks (sketches)\n",
    "    image_size = 256,           # image size\n",
    "    num_layers = 4,             # number of downsampling layers\n",
    "    num_resnet_blocks = 2,      # number of resnet blocks\n",
    "    vq_codebook_size = 1024,    # codebook size\n",
    "    vq_decay = 0.8 ,             # codebook exponential decay\n",
    "    use_hinge_loss = True,\n",
    "    use_vgg_and_gan = True\n",
    ").to(DEVICE)\n",
    "checkpoint = torch.load('/bulk/junzheyin/aaa/vae_epoch80', map_location = 'cpu')\n",
    "vae.load_state_dict(checkpoint, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "59dd9a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163 /30632, loss: 5.7510, evl: 0.0000\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/users/junzheyin/taming-transformers/nuwa_train_newvae-Copy1.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcondor.et.tudelft.nl/users/junzheyin/taming-transformers/nuwa_train_newvae-Copy1.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m latent_in \u001b[39m=\u001b[39m latent[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mto(DEVICE)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcondor.et.tudelft.nl/users/junzheyin/taming-transformers/nuwa_train_newvae-Copy1.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m latent_out \u001b[39m=\u001b[39m latent[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mto(DEVICE)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bcondor.et.tudelft.nl/users/junzheyin/taming-transformers/nuwa_train_newvae-Copy1.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m total_loss, loss, _, evt_loss_term \u001b[39m=\u001b[39m nuwa(video_input_latent \u001b[39m=\u001b[39;49m latent_in,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcondor.et.tudelft.nl/users/junzheyin/taming-transformers/nuwa_train_newvae-Copy1.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m     video_output_latent \u001b[39m=\u001b[39;49m latent_out, \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcondor.et.tudelft.nl/users/junzheyin/taming-transformers/nuwa_train_newvae-Copy1.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m     return_loss \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcondor.et.tudelft.nl/users/junzheyin/taming-transformers/nuwa_train_newvae-Copy1.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m     cond_dropout_prob \u001b[39m=\u001b[39;49m \u001b[39m0.1\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcondor.et.tudelft.nl/users/junzheyin/taming-transformers/nuwa_train_newvae-Copy1.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m     use_evt_loss \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcondor.et.tudelft.nl/users/junzheyin/taming-transformers/nuwa_train_newvae-Copy1.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m     evt_loss_weight \u001b[39m=\u001b[39;49m \u001b[39m0.75\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcondor.et.tudelft.nl/users/junzheyin/taming-transformers/nuwa_train_newvae-Copy1.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m (total_loss \u001b[39m/\u001b[39m ga)\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcondor.et.tudelft.nl/users/junzheyin/taming-transformers/nuwa_train_newvae-Copy1.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39mif\u001b[39;00m (i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m%\u001b[39mga \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/users/junzheyin/taming-transformers/nuwa_train_newvae-Copy1.ipynb Cell 11\u001b[0m in \u001b[0;36mnuwa_v2v.forward\u001b[0;34m(self, video_input_latent, video_output_latent, return_loss, cond_dropout_prob, use_evt_loss, evt_loss_weight)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bcondor.et.tudelft.nl/users/junzheyin/taming-transformers/nuwa_train_newvae-Copy1.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=182'>183</a>\u001b[0m \u001b[39mif\u001b[39;00m use_evt_loss:\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bcondor.et.tudelft.nl/users/junzheyin/taming-transformers/nuwa_train_newvae-Copy1.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=183'>184</a>\u001b[0m     ext_tokens \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([\u001b[39m452\u001b[39m, \u001b[39m303\u001b[39m, \u001b[39m171\u001b[39m, \u001b[39m653\u001b[39m, \u001b[39m891\u001b[39m, \u001b[39m603\u001b[39m, \u001b[39m499\u001b[39m, \u001b[39m160\u001b[39m, \u001b[39m216\u001b[39m, \u001b[39m109\u001b[39m, \u001b[39m290\u001b[39m, \u001b[39m797\u001b[39m, \u001b[39m956\u001b[39m, \u001b[39m607\u001b[39m, \u001b[39m816\u001b[39m])\u001b[39m.\u001b[39mto(DEVICE)\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bcondor.et.tudelft.nl/users/junzheyin/taming-transformers/nuwa_train_newvae-Copy1.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=184'>185</a>\u001b[0m     evt_loss_term \u001b[39m=\u001b[39m evt_loss_weight \u001b[39m*\u001b[39m evt_loss(rearrange(logits, \u001b[39m'\u001b[39;49m\u001b[39mb n c -> b c n\u001b[39;49m\u001b[39m'\u001b[39;49m),  \n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bcondor.et.tudelft.nl/users/junzheyin/taming-transformers/nuwa_train_newvae-Copy1.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=185'>186</a>\u001b[0m                                        video_output_latent\u001b[39m.\u001b[39;49msqueeze(\u001b[39m1\u001b[39;49m), \n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bcondor.et.tudelft.nl/users/junzheyin/taming-transformers/nuwa_train_newvae-Copy1.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=186'>187</a>\u001b[0m                                        ext_tokens, gamma \u001b[39m=\u001b[39;49m \u001b[39m1\u001b[39;49m, beta0 \u001b[39m=\u001b[39;49m \u001b[39m0.95\u001b[39;49m, beta1 \u001b[39m=\u001b[39;49m \u001b[39m0.05\u001b[39;49m, whole \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m)  \n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bcondor.et.tudelft.nl/users/junzheyin/taming-transformers/nuwa_train_newvae-Copy1.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=187'>188</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39misnan(evt_loss_term):\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bcondor.et.tudelft.nl/users/junzheyin/taming-transformers/nuwa_train_newvae-Copy1.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=188'>189</a>\u001b[0m         total_loss \u001b[39m=\u001b[39m loss \u001b[39m+\u001b[39m evt_loss_weight \u001b[39m*\u001b[39m  evt_loss_term    \n",
      "\u001b[1;32m/users/junzheyin/taming-transformers/nuwa_train_newvae-Copy1.ipynb Cell 11\u001b[0m in \u001b[0;36mevt_loss\u001b[0;34m(logits, gt_tokens, ext_tokens, gamma, beta0, beta1, whole)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcondor.et.tudelft.nl/users/junzheyin/taming-transformers/nuwa_train_newvae-Copy1.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39mfor\u001b[39;00m n \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(number):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcondor.et.tudelft.nl/users/junzheyin/taming-transformers/nuwa_train_newvae-Copy1.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=44'>45</a>\u001b[0m     gt \u001b[39m=\u001b[39m gt_tokens[batch_id,n]\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bcondor.et.tudelft.nl/users/junzheyin/taming-transformers/nuwa_train_newvae-Copy1.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=45'>46</a>\u001b[0m     \u001b[39mif\u001b[39;00m gt \u001b[39mnot\u001b[39;49;00m \u001b[39min\u001b[39;49;00m ext_tokens: \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcondor.et.tudelft.nl/users/junzheyin/taming-transformers/nuwa_train_newvae-Copy1.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=46'>47</a>\u001b[0m     gt_indicator \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcondor.et.tudelft.nl/users/junzheyin/taming-transformers/nuwa_train_newvae-Copy1.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=47'>48</a>\u001b[0m     prob \u001b[39m=\u001b[39m softmax(logits[batch_id,:,n])\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.8/site-packages/torch/_tensor.py:757\u001b[0m, in \u001b[0;36mTensor.__contains__\u001b[0;34m(self, element)\u001b[0m\n\u001b[1;32m    754\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[39m.\u001b[39m\u001b[39m__contains__\u001b[39m, (\u001b[39mself\u001b[39m,), \u001b[39mself\u001b[39m, element)\n\u001b[1;32m    755\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(element, (torch\u001b[39m.\u001b[39mTensor, Number)):\n\u001b[1;32m    756\u001b[0m     \u001b[39m# type hint doesn't understand the __contains__ result array\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     \u001b[39mreturn\u001b[39;00m (element \u001b[39m==\u001b[39;49m \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39many()\u001b[39m.\u001b[39mitem()  \u001b[39m# type: ignore[union-attr]\u001b[39;00m\n\u001b[1;32m    759\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    760\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mTensor.__contains__ only supports Tensor or scalar, but you passed in a \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m    761\u001b[0m     \u001b[39mtype\u001b[39m(element)\n\u001b[1;32m    762\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "#from pysteps.visualization import plot_precip_field\n",
    "nuwa.to(DEVICE)\n",
    "#torch.cuda.empty_cache()\n",
    "num_epochs = 10\n",
    "total_step = len(loaders_latent['train_aadedwre'])\n",
    "loss_sum = 0\n",
    "evl_sum = 0\n",
    "count = 0\n",
    "ga = 64\n",
    "number = 12 \n",
    "\n",
    "for g in optimizer.param_groups:\n",
    "    g['lr'] = 5e-4\n",
    "    g['weight_decay'] = 0.1\n",
    "for epoch in range(num_epochs):\n",
    "    for i, latent in enumerate(loaders_latent['train_aadedwre']):\n",
    "        nuwa.train()\n",
    "        torch.set_grad_enabled(True)\n",
    "        #if i<0:continue\n",
    "        #if i>=1:break\n",
    "        latent_in = latent[0].to(DEVICE)\n",
    "        latent_out = latent[1].to(DEVICE)\n",
    "        \n",
    "        total_loss, loss, _, evt_loss_term = nuwa(video_input_latent = latent_in,\n",
    "            video_output_latent = latent_out, \n",
    "            return_loss = True,\n",
    "            cond_dropout_prob = 0.1,\n",
    "            use_evt_loss = True,\n",
    "            evt_loss_weight = 0.75)\n",
    "        (total_loss / ga).backward()\n",
    "        \n",
    "        if (i+1)%ga == 0:\n",
    "            torch.nn.utils.clip_grad_norm_(nuwa.parameters(), 0.5)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        loss_sum += float(total_loss.item())\n",
    "        evl_sum += float(evt_loss_term)\n",
    "        count += 1\n",
    "        \n",
    "        # backpropagation, compute gradients   \n",
    "        print( (i+1) % total_step, '/{}, loss: {:.4f}, evl: {:.4f}'.format(total_step, loss_sum/count, evt_loss_term), end='\\r')\n",
    "        \n",
    "        if (i+1) % total_step == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Average Loss: {:.4f}' \n",
    "                   .format(epoch + 1, num_epochs, i + 1, total_step, loss_sum/count))\n",
    "            loss_sum = 0\n",
    "            count = 0 \n",
    "            # save check point\n",
    "            torch.save(nuwa.state_dict(),'/home/hbi/nuwa_newvae_sparse7_evl_75_epoch{}'.format(number))\n",
    "            torch.save(optimizer.state_dict(), '/home/hbi/nuwa_newvae_sparse7_evl_75_epoch{}_optim'.format(number))\n",
    "            for g in optimizer.param_groups:\n",
    "                g['lr'] = g['lr']*0.98\n",
    "            number += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2f6412f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(nuwa.state_dict(), '/home/hbi/nuwa_newvae_sparse7_evl_75_epoch13')\n",
    "torch.save(optimizer.state_dict(), '/home/hbi/nuwa_newvae_sparse7_evl_75_epoch13_optim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e88a9a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extreme Validation Average Loss:2.4409; Accuracy:35.2661 %\n"
     ]
    }
   ],
   "source": [
    "#checkpoint = torch.load( '/home/hbi/nuwa_newvae_epoch',map_location=\"cpu\")\n",
    "#nuwa.load_state_dict(checkpoint, strict=True)\n",
    "\n",
    "nuwa.to(DEVICE)\n",
    "nuwa.eval()\n",
    "loss_vali_sum = 0\n",
    "accuracy = 0\n",
    "for k, latent in enumerate(loaders_latent['test']):\n",
    "    print( (k+1), '/{}'.format(len(loaders_latent['test'])),end='\\r')\n",
    "    latent_in = latent[0].to(DEVICE)\n",
    "    latent_out = latent[1].to(DEVICE)\n",
    "    loss_vali, logits_vali = nuwa(video_input_latent = latent_in,\n",
    "            video_output_latent = latent_out, \n",
    "            return_loss = True)\n",
    "    indice_vali = torch.argmax(logits_vali, dim = 2)\n",
    "    latent_out = latent_out.squeeze(1)\n",
    "    accuracy += (latent_out==indice_vali).sum().item()*100/latent_out.shape[1]\n",
    "    loss_vali_sum += float(loss_vali.item())\n",
    "    # change to accuracy for validation test\n",
    "    if k == 626:\n",
    "        print('Extreme Validation Average Loss:{:.4f}; Accuracy:{:.4f} %'.format(loss_vali_sum/(k+1), accuracy/(k+1)))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3d688f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bfea79dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e974bfdd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
