{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5786c867",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n",
      "1.11.0\n",
      "11.3\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\".\")\n",
    "\n",
    "# also disable grad to save memory\n",
    "import torch\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "DEVICE = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "import yaml\n",
    "import torch\n",
    "from omegaconf import OmegaConf\n",
    "from taming.models.vqgan import VQModel, GumbelVQ\n",
    "import io\n",
    "import os, sys\n",
    "import requests\n",
    "import PIL\n",
    "from PIL import Image\n",
    "from PIL import ImageDraw, ImageFont\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f459a037",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from nuwa_pytorch import VQGanVAE\n",
    "from nuwa_pytorch.optimizer import get_optimizer\n",
    "vae = VQGanVAE(\n",
    "    dim = 256,\n",
    "    channels = 1,               # default is 3, but can be changed to any value for the training of the segmentation masks (sketches)\n",
    "    image_size = 256,           # image size\n",
    "    num_layers = 4,             # number of downsampling layers\n",
    "    num_resnet_blocks = 2,      # number of resnet blocks\n",
    "    vq_codebook_size = 1024,    # codebook size\n",
    "    vq_decay = 0.8 ,             # codebook exponential decay\n",
    "    use_hinge_loss = True,\n",
    "    use_vgg_and_gan = True\n",
    ").to(DEVICE)\n",
    "checkpoint = torch.load('/bulk/junzheyin/vaecheckpoint/vae_epoch80', map_location = 'cpu')\n",
    "vae.load_state_dict(checkpoint, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21543509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset\n",
    "import torch\n",
    "import sys\n",
    "from nuwa_pytorch import VQGanVAE\n",
    "import h5py\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "def eventGeneration(start_time, obs_time = 3 ,lead_time = 6, time_interval = 30):\n",
    "    # Generate event based on starting time point, return a list: [[t-4,...,t-1,t], [t+1,...,t+72]]\n",
    "    # Get the start year, month, day, hour, minute\n",
    "    year = int(start_time[0:4])\n",
    "    month = int(start_time[4:6])\n",
    "    day = int(start_time[6:8])\n",
    "    hour = int(start_time[8:10])\n",
    "    minute = int(start_time[10:12])\n",
    "    #print(datetime(year=year, month=month, day=day, hour=hour, minute=minute))\n",
    "    times = [(datetime(year, month, day, hour, minute) + timedelta(minutes = time_interval * (x+1))) for x in range(lead_time)]\n",
    "    lead = [dt.strftime('%Y%m%d%H%M') for dt in times]\n",
    "    times = [(datetime(year, month, day, hour, minute) - timedelta(minutes = time_interval * x)) for x in range(obs_time)]\n",
    "    obs = [dt.strftime('%Y%m%d%H%M') for dt in times]\n",
    "    obs.reverse()\n",
    "    return lead, obs\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import h5py\n",
    "import numpy as np\n",
    "from torchvision.transforms import ToTensor, Compose, CenterCrop\n",
    "class radarDataset(Dataset):\n",
    "    def __init__(self, root_dir, event_times, obs_number = 3, pred_number = 6, transform=None):\n",
    "        # event_times is an array of starting time t(string)\n",
    "        # transform is the preprocessing functions\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.event_times = event_times\n",
    "        self.obs_number = obs_number\n",
    "        self.pred_number = pred_number\n",
    "    def __len__(self):\n",
    "        return len(self.event_times)\n",
    "    def __getitem__(self, idx):\n",
    "        start_time = str(self.event_times[idx])\n",
    "        time_list_pre, time_list_obs = eventGeneration(start_time, self.obs_number, self.pred_number)\n",
    "        output = []\n",
    "        time_list = time_list_obs + time_list_pre\n",
    "        #print(time_list)\n",
    "        for time in time_list:\n",
    "            year = time[0:4]\n",
    "            month = time[4:6]\n",
    "            #path = self.root_dir + year + '/' + month + '/' + 'RAD_NL25_RAC_MFBS_EM_5min_' + time + '_NL.h5'\n",
    "            path = self.root_dir + year + '/' + month + '/' + 'RAD_NL25_RAP_5min_' + time + '.h5'\n",
    "            image = np.array(h5py.File(path)['image1']['image_data'])\n",
    "            #image = np.ma.masked_where(image == 65535, image)\n",
    "\n",
    "            image = image[264:520,242:498]\n",
    "            image[image == 65535] = 0\n",
    "            image = image.astype('float32')\n",
    "            image = image/100*12\n",
    "            image = np.clip(image, 0, 128)\n",
    "            image = image/40\n",
    "            output.append(image)\n",
    "        output = torch.permute(torch.tensor(np.array(output)), (1, 2, 0))\n",
    "        output = self.transform(np.array(output))\n",
    "        return output, start_time\n",
    "#root_dir = '/users/hbi/data/RAD_NL25_RAC_MFBS_EM_5min/'\n",
    "#dataset = radarDataset(root_dir, [\"200808031600\"], transform = Compose([ToTensor(),CenterCrop(256)]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5cf4c47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# develop dataset\n",
    "from torch.cuda.amp import autocast\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import pandas as pd\n",
    "root_dir = '/home/hbi/RAD_NL25_RAP_5min/' \n",
    "\n",
    "# M-L, top 5%\n",
    "df_train = pd.read_csv('/users/hbi/taming-transformers/training_Delfland08-14.csv', header = None)\n",
    "event_times = df_train[0].to_list()\n",
    "dataset_train = radarDataset(root_dir, event_times, transform = Compose([ToTensor()]))           \n",
    "\n",
    "df_test = pd.read_csv('/users/hbi/taming-transformers/testing_Delfland18-20.csv', header = None)\n",
    "event_times = df_test[0].to_list()\n",
    "dataset_test = radarDataset(root_dir, event_times, transform = Compose([ToTensor()]))\n",
    "\n",
    "df_vali = pd.read_csv('/users/hbi/taming-transformers/validation_Delfland15-17.csv', header = None)\n",
    "event_times = df_vali[0].to_list()\n",
    "dataset_vali = radarDataset(root_dir, event_times, transform = Compose([ToTensor()]))\n",
    "\n",
    "df_train_aa = pd.read_csv('/users/hbi/taming-transformers/training_Aa08-14.csv', header = None)\n",
    "event_times = df_train_aa[0].to_list()\n",
    "dataset_train_aa = radarDataset(root_dir, event_times, transform = Compose([ToTensor()]))    \n",
    "\n",
    "df_train_dw = pd.read_csv('/users/hbi/taming-transformers/training_Dwar08-14.csv', header = None)\n",
    "event_times = df_train_dw[0].to_list()\n",
    "dataset_train_dw = radarDataset(root_dir, event_times, transform = Compose([ToTensor()]))   \n",
    "\n",
    "df_train_re = pd.read_csv('/users/hbi/taming-transformers/training_Regge08-14.csv', header = None)\n",
    "event_times = df_train_re[0].to_list()\n",
    "dataset_train_re = radarDataset(root_dir, event_times, transform = Compose([ToTensor()]))    \n",
    "\n",
    "# extreme, top 1%\n",
    "df_train_ext = pd.read_csv('/users/hbi/taming-transformers/training_Delfland08-14_ext.csv', header = None)\n",
    "event_times = df_train_ext[0].to_list()\n",
    "mfbs = df_train_ext[1].to_list()\n",
    "dic_mfbs1 = dict(zip(event_times, mfbs))\n",
    "dataset_train_ext = radarDataset(root_dir, event_times, transform = Compose([ToTensor()]))           \n",
    "\n",
    "df_test_ext = pd.read_csv('/users/hbi/taming-transformers/testing_Delfland18-20_ext.csv', header = None)\n",
    "event_times = df_test_ext[0].to_list()\n",
    "mfbs = df_test_ext[1].to_list()\n",
    "dic_mfbs2 = dict(zip(event_times, mfbs))\n",
    "dataset_test_ext = radarDataset(root_dir, event_times, transform = Compose([ToTensor()]))\n",
    "\n",
    "df_vali_ext = pd.read_csv('/users/hbi/taming-transformers/validation_Delfland15-17_ext.csv', header = None)\n",
    "event_times = df_vali_ext[0].to_list()\n",
    "mfbs = df_vali_ext[1].to_list()\n",
    "dic_mfbs3 = dict(zip(event_times, mfbs))\n",
    "dataset_vali_ext = radarDataset(root_dir, event_times, transform = Compose([ToTensor()]))\n",
    "\n",
    "dic_mfbs = {}\n",
    "dic_mfbs.update(dic_mfbs1)\n",
    "dic_mfbs.update(dic_mfbs2)\n",
    "dic_mfbs.update(dic_mfbs3)\n",
    "new_valid = ['201606222325', '201508131920', '201606230050', '201606230100', '201606230200', '201508261735', '201508261800', '201708300255', '201708300300', '201707291755', '201707291800', '201606201210', '201708300155', '201606230300', '201707120520', '201707120600', '201707120455', '201708300055', '201707120700', '201605301745', '201509041820', '201605301800', '201709081725', '201606201300', '201708300420', '201709081650', '201509041755', '201709081545', '201709081455', '201709141200', '201709141155', '201709081800', '201508132000', '201707062000', '201707061955', '201711270745', '201709141300', '201709141055', '201605301655', '201709081900', '201708300500', '201805291515', '202006171920', '201805291455', '201809050545', '202006171855', '201809050600', '201906150240', '201906150300', '202008161555', '202008161600', '201805291600', '202006122025', '201906150155', '201906120820', '201906052100', '202006171755', '201906120755', '201808242240', '201804292310', '201906120940', '202007252055', '201804292255', '201808242300', '202007252100', '201906121000', '202006172000', '201809050700', '202002091830', '202009232020', '201808101930', '201808242050', '201906150400', '201808242100', '202002091900', '202006050545', '201910210430', '202007251955', '201808102000', '201804102045', '201804300000', '201804102100', '201906052200', '201808101820', '202006050600', '201810300150', '201906052300', '201810300200', '201804102200', '201808101755', '201910061250', '201810300300', '202009232100']\n",
    "new_valid.sort()\n",
    "#new_valid = new_valid[41:]\n",
    "#print(new_valid)\n",
    "        \n",
    "dataset_ext = radarDataset(root_dir, new_valid, transform = Compose([ToTensor()])) \n",
    "\n",
    "#print(\"Extreme:\", len(dataset_train_ext), len(dataset_test_ext), len(dataset_vali_ext))\n",
    "loaders = { 'test' :DataLoader(dataset_test_ext, batch_size=1, shuffle=True, num_workers=8), \n",
    "           'valid' :DataLoader(dataset_vali, batch_size=1, shuffle=False, num_workers=0),\n",
    "            'ext' :DataLoader(dataset_ext, batch_size=1, shuffle=False, num_workers=8),\n",
    "           'train_ext' :DataLoader(dataset_train_ext, batch_size=1, shuffle=True, num_workers=8),\n",
    "           'train_aa5' :DataLoader(dataset_train_aa, batch_size=1, shuffle=False, num_workers=0),\n",
    "           'train_del5' :DataLoader(dataset_train, batch_size=1, shuffle=True, num_workers=0),\n",
    "           'train_dw5' :DataLoader(dataset_train_dw, batch_size=1, shuffle=False, num_workers=0),\n",
    "           'train_re5' :DataLoader(dataset_train_re, batch_size=1, shuffle=False, num_workers=0)}\n",
    "\n",
    "#print(len(loaders['ext']))\n",
    "#print(dataset_ext[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7fa12f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from nuwa_pytorch.nuwa_pytorch import Sparse3DNA, Attention, SparseCross2DNA, cast_tuple, mult_reduce, calc_same_padding, unfoldNd, FeedForward, ShiftVideoTokens, SandwichNorm, StableLayerNorm\n",
    "class SparseCross3DNA(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        video_shape,\n",
    "        kernel_size = 3,\n",
    "        dilation = 1,\n",
    "        heads = 8,\n",
    "        dim_head = 64,\n",
    "        dropout = 0.,\n",
    "        causal = False,\n",
    "        query_num_frames_chunk = None,\n",
    "        rel_pos_bias = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head * heads\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.to_q = nn.Linear(dim, inner_dim, bias = False)\n",
    "        self.to_kv = nn.Linear(dim, inner_dim * 2, bias = False)\n",
    "        self.talking_heads = nn.Conv2d(heads, heads, 1, bias = False)\n",
    "        self.to_out = nn.Linear(inner_dim, dim)\n",
    "        self.dilation = cast_tuple(dilation, size = 3)\n",
    "        self.kernel_size = cast_tuple(kernel_size, size = 3)\n",
    "        self.kernel_numel = mult_reduce(self.kernel_size)\n",
    "       # relative positional bias per head, if needed\n",
    "        self.rel_pos_bias = AxialPositionalEmbedding(heads, shape = self.kernel_size) if rel_pos_bias else None\n",
    "        # calculate padding\n",
    "        self.padding_frame = calc_same_padding(self.kernel_size[0], self.dilation[0])\n",
    "        self.padding_height = calc_same_padding(self.kernel_size[1], self.dilation[1])\n",
    "        self.padding_width = calc_same_padding(self.kernel_size[2], self.dilation[2])\n",
    "        self.video_padding = (self.padding_width, self.padding_width, self.padding_height, self.padding_height, self.padding_frame, self.padding_frame)\n",
    "        # save video shape and calculate max number of tokens\n",
    "        self.video_shape = video_shape\n",
    "        max_frames, fmap_size, _ = video_shape\n",
    "        max_num_tokens = torch.empty(video_shape).numel()\n",
    "        self.max_num_tokens = max_num_tokens\n",
    "        # how many query tokens to process at once to limit peak memory usage, by multiple of frame tokens (fmap_size ** 2)\n",
    "        self.query_num_frames_chunk = default(query_num_frames_chunk, max_frames)\n",
    "        # precalculate causal mask\n",
    "        indices = torch.arange(max_num_tokens)\n",
    "        shaped_indices = rearrange(indices, '(f h w) -> 1 1 f h w', f = max_frames, h = fmap_size, w = fmap_size)\n",
    "        padded_indices = F.pad(shaped_indices, self.video_padding, value = max_num_tokens) # padding has value of max tokens so to be masked out\n",
    "        unfolded_indices = unfoldNd(padded_indices, kernel_size = self.kernel_size, dilation = self.dilation)\n",
    "        unfolded_indices = rearrange(unfolded_indices, '1 k n -> n k')\n",
    "        # if causal, compare query and key indices and make sure past cannot see future\n",
    "        # if not causal, just mask out the padding\n",
    "\n",
    "        if causal:\n",
    "            mask = rearrange(indices, 'n -> n 1') < unfolded_indices\n",
    "        else:\n",
    "            mask = unfolded_indices == max_num_tokens\n",
    "\n",
    "        #mask = F.pad(mask, (1, 0), value = False) # bos tokens never get masked out\n",
    "        self.register_buffer('mask', mask)\n",
    "        #print(self.mask.shape)\n",
    "\n",
    "    def forward(self, x, context, **kwargs):\n",
    "        b, n, _, h, device = *x.shape, self.heads, x.device\n",
    "        n_context = context.shape[1]\n",
    "        # more variables\n",
    "\n",
    "        dilation = self.dilation\n",
    "        kernel_size = self.kernel_size\n",
    "        video_padding = self.video_padding\n",
    "        fmap_size = self.video_shape[1]\n",
    "        tokens_per_frame = fmap_size ** 2\n",
    "\n",
    "        padding = padding_to_multiple_of(n - 1, tokens_per_frame)\n",
    "        num_frames = (n + padding) // tokens_per_frame\n",
    "        num_frames_input = (n_context + padding) // tokens_per_frame\n",
    "        \n",
    "        # derive queries / keys / values\n",
    "\n",
    "        q, k, v = (self.to_q(x), *self.to_kv(context).chunk(2, dim = -1))\n",
    "        \n",
    "        bos_only = n == 1\n",
    "        if bos_only:\n",
    "            return self.to_out(v)\n",
    "        # split out heads\n",
    "\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> (b h) n d', h = h), (q, k, v))\n",
    "        \n",
    "       \n",
    "        # take care of bos\n",
    "\n",
    "        #q = q[:, 1:]\n",
    "        #bos_value = q[:, :1]\n",
    "        \n",
    "        # scale queries\n",
    "        q = q * self.scale\n",
    "\n",
    "        # reshape keys and values to video and add appropriate padding along all dimensions (frames, height, width)\n",
    "        k, v = map(lambda t: rearrange(t, 'b (f h w) d -> b d f h w',  h = fmap_size, w = fmap_size), (k, v))\n",
    "        k, v = map(lambda t: F.pad(t, video_padding), (k, v))\n",
    "        #print(k.shape)\n",
    "        # axial relative pos bias\n",
    "\n",
    "        rel_pos_bias = None\n",
    "\n",
    "        if exists(self.rel_pos_bias):\n",
    "            rel_pos_bias = rearrange(self.rel_pos_bias(), 'j h -> h 1 j')\n",
    "            rel_pos_bias = F.pad(rel_pos_bias, (1, 0), value = 0.)\n",
    "\n",
    "        # put the attention processing code in a function\n",
    "        # to allow for processing queries in chunks of frames\n",
    "\n",
    "        out = []\n",
    "\n",
    "        def attend(q, k, v, mask, kernel_size):\n",
    "            chunk_length = q.shape[1]\n",
    "            k, v = map(lambda t: unfoldNd(t, kernel_size = kernel_size, dilation = dilation), (k, v))\n",
    "            k, v = map(lambda t: rearrange(t, 'b (d j) i -> b i j d', j = self.kernel_numel), (k, v))\n",
    "            k, v = map(lambda t: t[:, :chunk_length], (k, v))\n",
    "\n",
    "            # calculate sim\n",
    "\n",
    "            sim = einsum('b i d, b i j d -> b i j', q, k)\n",
    "            #print(q.shape, k.shape, sim.shape)\n",
    "            # add rel pos bias, if needed\n",
    "\n",
    "            if exists(rel_pos_bias):\n",
    "                sim = sim + rel_pos_bias\n",
    "\n",
    "            # causal mask\n",
    "\n",
    "            \n",
    "            if exists(mask):\n",
    "                mask_value = -torch.finfo(sim.dtype).max               \n",
    "                mask = rearrange(mask, 'i j -> 1 i j')\n",
    "                sim = sim.masked_fill(mask, mask_value)\n",
    "            \n",
    "            # attention\n",
    "\n",
    "            attn = stable_softmax(sim, dim = -1)\n",
    "\n",
    "            attn = rearrange(attn, '(b h) ... -> b h ...', h = h)\n",
    "            attn = self.talking_heads(attn)\n",
    "            attn = rearrange(attn, 'b h ... -> (b h) ...')\n",
    "\n",
    "            attn = self.dropout(attn)\n",
    "\n",
    "            # aggregate values\n",
    "\n",
    "            return einsum('b i j, b i j d -> b i d', attn, v)\n",
    "\n",
    "        # process queries in chunks\n",
    "\n",
    "        frames_per_chunk = num_frames_input\n",
    "        chunk_size = frames_per_chunk * tokens_per_frame\n",
    "        q_chunks = q.split(chunk_size, dim = 1)\n",
    "        mask = self.mask\n",
    "        #print(q.shape, chunk_size)\n",
    "        for ind, q_chunk in enumerate(q_chunks):\n",
    "            #print(ind, q_chunks[ind].shape, mask.shape)\n",
    "            q_chunk = q_chunks[ind]\n",
    "            length = q_chunk.shape[1]\n",
    "            q_chunk = F.pad(input=q_chunk, pad=(0, 0, 0, chunk_size-length), mode='constant', value=float(\"-Inf\"))\n",
    "            mask_chunk = mask\n",
    "\n",
    "            # slice the keys and values to the appropriate frames, accounting for padding along frames dimension\n",
    "\n",
    "            k_slice, v_slice = k,v\n",
    "            # calculate output chunk\n",
    "            out_chunk = attend(\n",
    "                q = q_chunk,\n",
    "                k = k_slice,\n",
    "                v = v_slice,\n",
    "                mask = mask_chunk,\n",
    "                kernel_size = kernel_size,\n",
    "            )\n",
    "\n",
    "            out_chunk = out_chunk[:,:length,:]\n",
    "            out.append(out_chunk)\n",
    "            \n",
    "        # combine all chunks\n",
    "        out = torch.cat(out, dim = 1)\n",
    "        # append bos value\n",
    "\n",
    "        #out = torch.cat((bos_value, out), dim = 1)  # bos will always adopt its own value, since it pays attention only to itself\n",
    "        # merge heads\n",
    "\n",
    "        out = rearrange(out, '(b h) n d -> b n (h d)', h = h)\n",
    "        return self.to_out(out)\n",
    "    \n",
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        dim,\n",
    "        depth,\n",
    "        causal = False,\n",
    "        heads = 8,\n",
    "        dim_head = 64,\n",
    "        ff_mult = 4,\n",
    "        cross_attend = False,\n",
    "        attn_dropout = 0.,\n",
    "        ff_dropout = 0.,\n",
    "        ff_chunk_size = None,\n",
    "        cross_2dna_attn = False,\n",
    "        cross_2dna_image_size = None,\n",
    "        cross_2dna_kernel_size = 3,\n",
    "        cross_2dna_dilations = (1,),\n",
    "        cross_3dna_attn = False,\n",
    "        cross_3dna_image_size = None,\n",
    "        cross_3dna_kernel_size = 3,\n",
    "        cross_3dna_dilations = (1,),\n",
    "        sparse_3dna_attn = False,\n",
    "        sparse_3dna_kernel_size = 3,\n",
    "        sparse_3dna_video_shape = None,\n",
    "        sparse_3dna_query_num_frames_chunk = None,\n",
    "        sparse_3dna_dilations = (1,),\n",
    "        sparse_3dna_rel_pos_bias = False,\n",
    "        shift_video_tokens = False,\n",
    "        rotary_pos_emb = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = MList([])\n",
    "\n",
    "        for ind in range(depth):\n",
    "            if sparse_3dna_attn:\n",
    "                dilation = sparse_3dna_dilations[ind % len(sparse_3dna_dilations)]\n",
    "\n",
    "                self_attn = Sparse3DNA(\n",
    "                    dim = dim,\n",
    "                    heads = heads,\n",
    "                    dim_head = dim_head,\n",
    "                    causal = causal,\n",
    "                    kernel_size = sparse_3dna_kernel_size,\n",
    "                    dilation = dilation,\n",
    "                    video_shape = sparse_3dna_video_shape,\n",
    "                    query_num_frames_chunk = sparse_3dna_query_num_frames_chunk,\n",
    "                    rel_pos_bias = sparse_3dna_rel_pos_bias,\n",
    "                )\n",
    "            else:\n",
    "                self_attn = Attention(\n",
    "                    dim = dim,\n",
    "                    heads = heads,\n",
    "                    dim_head = dim_head,\n",
    "                    causal = causal,\n",
    "                    dropout = attn_dropout\n",
    "                )\n",
    "\n",
    "            cross_attn = None\n",
    "            \n",
    "            if cross_attend:\n",
    "                if cross_2dna_attn:\n",
    "                    dilation = cross_2dna_dilations[ind % len(cross_2dna_dilations)]\n",
    "\n",
    "                    cross_attn = SparseCross2DNA(\n",
    "                        dim = dim,\n",
    "                        heads = heads,\n",
    "                        dim_head = dim_head,\n",
    "                        dropout = attn_dropout,\n",
    "                        image_size = cross_2dna_image_size,\n",
    "                        kernel_size = cross_2dna_kernel_size,\n",
    "                        dilation = dilation\n",
    "                    )\n",
    "                \n",
    "                elif cross_3dna_attn:\n",
    "                    \n",
    "                    cross_attn = SparseCross3DNA(\n",
    "                        dim = dim,\n",
    "                        heads = heads,\n",
    "                        dim_head = dim_head,\n",
    "                        dropout = attn_dropout,\n",
    "                        video_shape = cross_3dna_image_size,\n",
    "                        kernel_size = cross_3dna_kernel_size,\n",
    "                    )\n",
    "                \n",
    "                else:\n",
    "                    cross_attn = Attention(\n",
    "                        dim = dim,\n",
    "                        heads = heads,\n",
    "                        dim_head = dim_head,\n",
    "                        dropout = attn_dropout\n",
    "                    )\n",
    "\n",
    "            ff = FeedForward(dim = dim, mult = ff_mult, dropout = ff_dropout, chunk_size = ff_chunk_size)\n",
    "\n",
    "            if sparse_3dna_attn and shift_video_tokens:\n",
    "                fmap_size = sparse_3dna_video_shape[-1]\n",
    "                self_attn = ShiftVideoTokens(self_attn, image_size = fmap_size)\n",
    "                ff        = ShiftVideoTokens(ff, image_size = fmap_size)\n",
    "\n",
    "            self.layers.append(MList([\n",
    "                SandwichNorm(dim = dim, fn = self_attn),\n",
    "                SandwichNorm(dim = dim, fn = cross_attn) if cross_attend else None,\n",
    "                SandwichNorm(dim = dim, fn = ff)\n",
    "            ]))\n",
    "\n",
    "        self.norm = StableLayerNorm(dim)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x,\n",
    "        mask = None,\n",
    "        context = None,\n",
    "        context_mask = None\n",
    "    ):\n",
    "        for attn, cross_attn, ff in self.layers:\n",
    "            x = attn(x, mask = mask) + x\n",
    "\n",
    "            if exists(cross_attn):\n",
    "                x = cross_attn(x, context = context, mask = mask, context_mask = context_mask) + x\n",
    "\n",
    "            x = ff(x) + x\n",
    "\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c75aa3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer for second stage\n",
    "# transformer for second stage\n",
    "from torch import nn\n",
    "import nuwa_pytorch.nuwa_pytorch\n",
    "from nuwa_pytorch.nuwa_pytorch import MList, Embedding, AxialPositionalEmbedding, ReversibleTransformer, default, eval_decorator, prob_mask_like, exists\n",
    "from nuwa_pytorch.nuwa_pytorch import padding_to_multiple_of, einsum, top_k\n",
    "from einops import rearrange, repeat\n",
    "import torch.nn.functional as F\n",
    "from main import instantiate_from_config\n",
    "from nuwa_pytorch.vqgan_vae import stable_softmax\n",
    "#from dice_loss import DiceLoss\n",
    "class nuwa_v2v(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        dim,\n",
    "        vae = None,\n",
    "        image_size = 256,\n",
    "        codebook_size = 1024,\n",
    "        compression_rate = 16,\n",
    "        video_out_seq_len = 5,\n",
    "        video_in_seq_len = 5,\n",
    "        video_dec_depth = 6,\n",
    "        video_dec_dim_head = 64,\n",
    "        video_dec_heads = 8,\n",
    "        attn_dropout = 0.,\n",
    "        ff_dropout = 0.,\n",
    "        ff_chunk_size = None,\n",
    "        embed_gradient_frac = 0.2,\n",
    "        shift_video_tokens = True,\n",
    "        sparse_3dna_kernel_size = 3,\n",
    "        sparse_3dna_query_num_frames_chunk = None,\n",
    "        sparse_3dna_dilation = 1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # VAE\n",
    "        #self.first_stage_model = vae.copy_for_eval()\n",
    "        self.image_size = image_size\n",
    "        num_image_tokens = codebook_size\n",
    "        fmap_size = image_size // compression_rate\n",
    "        self.video_fmap_size = fmap_size\n",
    "        self.video_frames_in = video_in_seq_len\n",
    "        self.video_frames_out = video_out_seq_len\n",
    "        self.image_embedding = Embedding(num_image_tokens, dim, frac_gradient=embed_gradient_frac)\n",
    "        # cycle dilation for sparse 3d-nearby attention \n",
    "        sparse_3dna_dilations = tuple(range(1, sparse_3dna_dilation + 1)) if not isinstance(sparse_3dna_dilation, (list, tuple)) else sparse_3dna_dilation  # ???\n",
    "        \n",
    "        video_shape_in = (video_in_seq_len, fmap_size, fmap_size)\n",
    "        video_shape_out = (video_out_seq_len, fmap_size, fmap_size)\n",
    "        self.video_in_pos_emb = AxialPositionalEmbedding(dim, shape = video_shape_in)\n",
    "        self.video_out_pos_emb = AxialPositionalEmbedding(dim, shape = video_shape_out)\n",
    "        self.video_bos = nn.Parameter(torch.randn(dim))\n",
    "        \n",
    "        # 3DNA Encoder\n",
    "        video_shape_input = (video_in_seq_len, fmap_size, fmap_size)\n",
    "    \n",
    "        # 3DNA Decoder\n",
    "        #self.video_bos = nn.Parameter(torch.randn(dim))#\n",
    "        #self.image_embedding = Embedding(num_image_tokens, dim, frac_gradient = embed_gradient_frac)#???\n",
    "        # self.video_pos_emb = AxialPositionalEmbedding(dim, shape = video_shape)#???\n",
    "\n",
    "        video_shape_output = (video_out_seq_len, fmap_size, fmap_size)\n",
    "        self.video_decode_transformer = Transformer(\n",
    "            dim = dim,\n",
    "            depth = video_dec_depth,\n",
    "            heads = video_dec_heads,\n",
    "            dim_head = video_dec_dim_head,\n",
    "            causal = True,\n",
    "            cross_attend = True,\n",
    "            attn_dropout = attn_dropout,\n",
    "            ff_dropout = ff_dropout,\n",
    "            ff_chunk_size = ff_chunk_size,\n",
    "            cross_3dna_attn = True,\n",
    "            cross_3dna_image_size = video_shape_input,\n",
    "            cross_3dna_kernel_size = (5,3,3),\n",
    "            shift_video_tokens = shift_video_tokens,\n",
    "            sparse_3dna_video_shape = video_shape_output,\n",
    "            sparse_3dna_attn = True,\n",
    "            sparse_3dna_kernel_size = sparse_3dna_kernel_size,\n",
    "            sparse_3dna_dilations = sparse_3dna_dilations,\n",
    "            sparse_3dna_query_num_frames_chunk = sparse_3dna_query_num_frames_chunk\n",
    "        )\n",
    "\n",
    "        self.to_logits = nn.Linear(dim, num_image_tokens)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    @eval_decorator\n",
    "    def generate(\n",
    "        self,\n",
    "        *,\n",
    "        video_input_latent,\n",
    "        video_output_latent = None,\n",
    "        filter_thres = 0.9,\n",
    "        temperature = 1.,\n",
    "        num_frames = 6,\n",
    "        prior_weight = 1,\n",
    "        alpha = 1\n",
    "    ):\n",
    "        #video_indice_last_frame = video_input_latent[0,:,512:768]\n",
    "        device = video_input_latent.device\n",
    "        batch = video_input_latent.shape[0]\n",
    "        \n",
    "        frame_indices_input = video_input_latent.squeeze(1)\n",
    "        frame_embeddings_input = self.image_embedding(frame_indices_input)\n",
    "        frame_embeddings_input = self.video_in_pos_emb().repeat(batch,1,1) + frame_embeddings_input\n",
    "        \n",
    "        bos = repeat(self.video_bos, 'd -> b 1 d', b = batch)\n",
    "        video_indices = torch.empty((batch, 0), device = device, dtype = torch.long)\n",
    "        num_tokens_per_frame = self.video_fmap_size ** 2\n",
    "        \n",
    "        num_frames = default(num_frames, self.video_frames_out)\n",
    "        total_video_tokens =  num_tokens_per_frame * num_frames\n",
    "        \n",
    "        pos_emb = self.video_out_pos_emb()\n",
    "        \n",
    "        prior = frame_indices_input[:, 512:].repeat(1,num_frames)\n",
    "        for ind in range(total_video_tokens):\n",
    "            print(ind, '/', total_video_tokens, end = '\\r')\n",
    "            \n",
    "            video_indices_input = video_indices\n",
    "            num_video_tokens = video_indices.shape[1]\n",
    "            #video_indices_input = video_output_latent[:,: num_video_tokens]\n",
    "            \n",
    "            frame_embeddings = self.image_embedding(video_indices_input)\n",
    "            frame_embeddings = pos_emb[:frame_embeddings.shape[1]] + frame_embeddings\n",
    "            frame_embeddings = torch.cat((bos, frame_embeddings), dim = 1)\n",
    "            \n",
    "            frame_embeddings = self.video_decode_transformer(\n",
    "                frame_embeddings,\n",
    "                context = frame_embeddings_input\n",
    "            )\n",
    "            \n",
    "        \n",
    "            logits = self.to_logits(frame_embeddings)\n",
    "            logits = logits[:, -1, :]\n",
    "            logits[:,int(prior[:,ind])] *= prior_weight\n",
    "            filtered_logits = top_k(logits, thres = filter_thres)\n",
    "            \n",
    "            #probs = F.softmax(filtered_logits, dim=-1)\n",
    "            #sample = torch.multinomial(probs, 1).squeeze(-1)\n",
    "            \n",
    "            sample = gumbel_sample(filtered_logits, temperature = temperature, dim = -1)\n",
    "            sample = rearrange(sample, 'b -> b 1')\n",
    "            video_indices = torch.cat((video_indices, sample), dim = 1)\n",
    "            \n",
    "            if (ind+1) % num_tokens_per_frame == 0:\n",
    "                n =  int((ind+1) / num_tokens_per_frame)\n",
    "                prior = video_indices[:, (n-1)*int(num_tokens_per_frame):].repeat(1,num_frames)\n",
    "                prior_weight = max(prior_weight*alpha, 1)\n",
    "        \n",
    "        return video_indices\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        *,\n",
    "        #video_input, # raw observation video here\n",
    "        #video_output, # raw prediction video here\n",
    "        video_input_latent,\n",
    "        video_output_latent,\n",
    "        return_loss = False,\n",
    "        cond_dropout_prob = 0\n",
    "    ):\n",
    "        device = video_input_latent.device\n",
    "        batch = video_input_latent.shape[0]\n",
    "        seq_out_len = self.video_frames_out\n",
    "        seq_in_len = self.video_frames_in\n",
    "        \n",
    "        #get indices\n",
    "        frame_indices_input = video_input_latent.squeeze(1)\n",
    "        frame_indices_output = video_output_latent.squeeze(1)\n",
    "        frame_indices_output = frame_indices_output[:, :-1] if return_loss else frame_indices_output\n",
    "        \n",
    "        #indices to embedding\n",
    "        frame_embeddings_input = self.image_embedding(frame_indices_input)\n",
    "        frame_embeddings_prediction = self.image_embedding(frame_indices_output)\n",
    "\n",
    "        #position encoding\n",
    "        frame_embeddings_input = self.video_in_pos_emb().repeat(batch,1,1) + frame_embeddings_input\n",
    "        if return_loss:\n",
    "            frame_embeddings_prediction = self.video_out_pos_emb()[:-1].repeat(batch,1,1) + frame_embeddings_prediction\n",
    "            bos = repeat(self.video_bos, 'd -> b 1 d', b = batch)\n",
    "            frame_embeddings_prediction = torch.cat((bos, frame_embeddings_prediction), dim = 1)\n",
    "        else:\n",
    "            frame_embeddings_prediction = self.video_out_pos_emb().repeat(batch,1,1) + frame_embeddings_prediction\n",
    "        \n",
    "        #transformer \n",
    "        frame_embeddings_prediction = self.video_decode_transformer(\n",
    "            frame_embeddings_prediction,\n",
    "            context = frame_embeddings_input\n",
    "        )\n",
    "        #print(frame_embeddings_prediction.shape)\n",
    "        logits = self.to_logits(frame_embeddings_prediction)\n",
    "        \n",
    "        #print(t2-t1, t3-t2, t4-t3, t5-t4)\n",
    "        if not return_loss:\n",
    "            return logits\n",
    "        #weight = torch.load('weight_clip_10.pt').to(device)\n",
    "        loss = F.cross_entropy(rearrange(logits, 'b n c -> b c n'), video_output_latent.squeeze(1))\n",
    "        \n",
    "        #dice_loss = DiceLoss(smooth = 1, with_logits = True, ohem_ratio = 0, \n",
    "        #                     alpha = 0.01, reduction = \"mean\",  index_label_position=True,  square_denominator = True)\n",
    "        #print(rearrange(logits, 'b n c -> (b n) c').shape, rearrange(video_output_latent.squeeze(1), 'b n -> (b n)').shape)\n",
    "        #loss = dice_loss(rearrange(logits, 'b n c -> (b n) c'), rearrange(video_output_latent.squeeze(1), 'b n -> (b n)'))\n",
    "        \n",
    "        return loss, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1b77d57",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#torch.cuda.empty_cache()\n",
    "nuwa = nuwa_v2v(dim=512, vae=None,\n",
    "                image_size = 256,\n",
    "                codebook_size = 1024,\n",
    "                compression_rate = 16,\n",
    "                video_out_seq_len = 6,\n",
    "                video_in_seq_len = 3,\n",
    "                video_dec_depth = 12,\n",
    "                video_dec_dim_head = 64,\n",
    "                video_dec_heads = 4,\n",
    "                sparse_3dna_kernel_size = (7,3,3),\n",
    "                attn_dropout = 0,\n",
    "                ff_dropout = 0).to(DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0efa866b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pysteps configuration file found at: /users/junzheyin/anaconda3/envs/myenv/lib/python3.8/site-packages/pysteps/pystepsrc\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pysteps.verification.detcatscores import det_cat_fct\n",
    "from pysteps.verification.detcontscores import det_cont_fct\n",
    "from pysteps.verification.spatialscores import intensity_scale\n",
    "from pysteps.visualization import plot_precip_field\n",
    "from nuwa_pytorch.nuwa_pytorch import top_k, gumbel_sample\n",
    "import random\n",
    "device = DEVICE\n",
    "nuwa.eval()\n",
    "loaders = { 'test' :DataLoader(dataset_test_ext, batch_size=1, shuffle=True, num_workers=8), \n",
    "           'valid' :DataLoader(dataset_vali, batch_size=1, shuffle=False, num_workers=0),\n",
    "            'ext' :DataLoader(dataset_ext, batch_size=1, shuffle=False, num_workers=8),\n",
    "           'train_ext' :DataLoader(dataset_train_ext, batch_size=1, shuffle=True, num_workers=8),\n",
    "           'train_aa5' :DataLoader(dataset_train_aa, batch_size=1, shuffle=False, num_workers=0),\n",
    "           'train_del5' :DataLoader(dataset_train, batch_size=1, shuffle=True, num_workers=0),\n",
    "           'train_dw5' :DataLoader(dataset_train_dw, batch_size=1, shuffle=False, num_workers=0),\n",
    "           'train_re5' :DataLoader(dataset_train_re, batch_size=1, shuffle=False, num_workers=0)}\n",
    "#torch.cuda.empty_cache()\n",
    "from collections import Counter\n",
    "import time\n",
    "from pysteps.verification.detcatscores import det_cat_fct\n",
    "from pysteps.verification.detcontscores import det_cont_fct\n",
    "from pysteps.verification.spatialscores import intensity_scale\n",
    "from pysteps.visualization import plot_precip_field\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "135e9dc8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201508131920\n",
      "1535 / 1536\r"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [4096, 2048, 3, 3], expected input[6, 256, 16, 16] to have 2048 channels, but got 256 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/users/junzheyin/taming-transformers/nuwa_date_newvae-Copy5.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcondor.et.tudelft.nl/users/junzheyin/taming-transformers/nuwa_date_newvae-Copy5.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=48'>49</a>\u001b[0m     video_generate1 \u001b[39m=\u001b[39m nuwa\u001b[39m.\u001b[39mgenerate(video_input_latent \u001b[39m=\u001b[39m indice_obs, \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcondor.et.tudelft.nl/users/junzheyin/taming-transformers/nuwa_date_newvae-Copy5.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=49'>50</a>\u001b[0m                                     video_output_latent \u001b[39m=\u001b[39m indice_pre,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcondor.et.tudelft.nl/users/junzheyin/taming-transformers/nuwa_date_newvae-Copy5.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=50'>51</a>\u001b[0m                                     filter_thres \u001b[39m=\u001b[39m \u001b[39m0.98\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcondor.et.tudelft.nl/users/junzheyin/taming-transformers/nuwa_date_newvae-Copy5.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=53'>54</a>\u001b[0m                                     prior_weight \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcondor.et.tudelft.nl/users/junzheyin/taming-transformers/nuwa_date_newvae-Copy5.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=54'>55</a>\u001b[0m                                     alpha \u001b[39m=\u001b[39m \u001b[39m0.8\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcondor.et.tudelft.nl/users/junzheyin/taming-transformers/nuwa_date_newvae-Copy5.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=55'>56</a>\u001b[0m \u001b[39m#_, video_generate1 = nuwa(video_input_latent= indice_obs,video_output_latent= indice_pre, return_loss = True)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcondor.et.tudelft.nl/users/junzheyin/taming-transformers/nuwa_date_newvae-Copy5.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=56'>57</a>\u001b[0m \u001b[39m#video_generate1 = torch.argmax(video_generate1,dim=-1)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcondor.et.tudelft.nl/users/junzheyin/taming-transformers/nuwa_date_newvae-Copy5.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=57'>58</a>\u001b[0m \u001b[39m#accuracy = (indice_pre==video_generate1).sum().item()*100/indice_pre.shape[1]\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bcondor.et.tudelft.nl/users/junzheyin/taming-transformers/nuwa_date_newvae-Copy5.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=59'>60</a>\u001b[0m     video_predict1 \u001b[39m=\u001b[39m vae\u001b[39m.\u001b[39;49mcodebook_indices_to_video(video_generate1)\u001b[39m.\u001b[39msqueeze(\u001b[39m0\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcondor.et.tudelft.nl/users/junzheyin/taming-transformers/nuwa_date_newvae-Copy5.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=60'>61</a>\u001b[0m \u001b[39m#video_predict2 = vae.codebook_indices_to_video(video_generate2).squeeze(0)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcondor.et.tudelft.nl/users/junzheyin/taming-transformers/nuwa_date_newvae-Copy5.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=61'>62</a>\u001b[0m \u001b[39m#indice_obs = torch.cat((indice_obs[:,256:], video_generate1), dim = 1)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcondor.et.tudelft.nl/users/junzheyin/taming-transformers/nuwa_date_newvae-Copy5.ipynb#X12sdnNjb2RlLXJlbW90ZQ%3D%3D?line=62'>63</a>\u001b[0m     vp \u001b[39m=\u001b[39m video_predict1[:,\u001b[39m0\u001b[39m,:,:]\u001b[39m.\u001b[39mto(\u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy()\u001b[39m*\u001b[39m\u001b[39m40\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.8/site-packages/nuwa_pytorch/vqgan_vae.py:34\u001b[0m, in \u001b[0;36meval_decorator.<locals>.inner\u001b[0;34m(model, *args, **kwargs)\u001b[0m\n\u001b[1;32m     32\u001b[0m was_training \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mtraining\n\u001b[1;32m     33\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[0;32m---> 34\u001b[0m out \u001b[39m=\u001b[39m fn(model, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     35\u001b[0m model\u001b[39m.\u001b[39mtrain(was_training)\n\u001b[1;32m     36\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.8/site-packages/nuwa_pytorch/vqgan_vae.py:449\u001b[0m, in \u001b[0;36mVQGanVAE.codebook_indices_to_video\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    447\u001b[0m codes \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcodebook[indices]\n\u001b[1;32m    448\u001b[0m codes \u001b[39m=\u001b[39m rearrange(codes, \u001b[39m'\u001b[39m\u001b[39mb (f h w) d -> (b f) d h w\u001b[39m\u001b[39m'\u001b[39m, h \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfmap_size, w \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfmap_size)\n\u001b[0;32m--> 449\u001b[0m video \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecode(codes)\n\u001b[1;32m    450\u001b[0m \u001b[39mreturn\u001b[39;00m rearrange(video, \u001b[39m'\u001b[39m\u001b[39m(b f) ... -> b f ...\u001b[39m\u001b[39m'\u001b[39m, b \u001b[39m=\u001b[39m b)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.8/site-packages/nuwa_pytorch/vqgan_vae.py:439\u001b[0m, in \u001b[0;36mVQGanVAE.decode\u001b[0;34m(self, fmap)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(\u001b[39mself\u001b[39m, fmap):\n\u001b[1;32m    438\u001b[0m     \u001b[39mfor\u001b[39;00m dec \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoders:\n\u001b[0;32m--> 439\u001b[0m         fmap \u001b[39m=\u001b[39m dec(fmap)\n\u001b[1;32m    441\u001b[0m     \u001b[39mreturn\u001b[39;00m fmap\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.8/site-packages/nuwa_pytorch/vqgan_vae.py:226\u001b[0m, in \u001b[0;36mGLUResBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m--> 226\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnet(x) \u001b[39m+\u001b[39m x\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.8/site-packages/torch/nn/modules/container.py:141\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    140\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 141\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.8/site-packages/torch/nn/modules/conv.py:447\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 447\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv/lib/python3.8/site-packages/torch/nn/modules/conv.py:443\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    440\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    441\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    442\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 443\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    444\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [4096, 2048, 3, 3], expected input[6, 256, 16, 16] to have 2048 channels, but got 256 channels instead"
     ]
    }
   ],
   "source": [
    "pcc_sum = [0]*6\n",
    "mse_sum = [0]*6\n",
    "mae_sum = [0]*6\n",
    "csi1_sum = [0]*6\n",
    "csi2_sum = [0]*6\n",
    "csi8_sum = [0]*6\n",
    "far1_sum = [0]*6\n",
    "far2_sum = [0]*6\n",
    "far8_sum = [0]*6\n",
    "fss1_sum = [0]*6\n",
    "fss10_sum = [0]*6\n",
    "fss20_sum = [0]*6\n",
    "fss30_sum = [0]*6\n",
    "record = {}\n",
    "name = '/home/hbi/predictions_new_evl_1/{}.npy'\n",
    "checkpoint = torch.load( '/bulk/junzheyin/nuwa_newvae_sparse7_evl_75_epoch12',map_location=\"cpu\")\n",
    "nuwa.load_state_dict(checkpoint, strict=True)\n",
    "#for name in ['/home/hbi/predictions_evl100/{}.npy', '/home/hbi/predictions_evl75/{}.npy', '/home/hbi/predictions_evl50/{}.npy', '/home/hbi/predictions_ce/{}.npy', '/home/hbi/predictions_wce/{}.npy']:\n",
    "#    print(name)\n",
    "count = 0\n",
    "count_nan = 0\n",
    "j = 0\n",
    "for i, (images,time) in enumerate(loaders['ext']):\n",
    "    print(time[0])\n",
    "    image = images[0].unsqueeze(1).unsqueeze(0)\n",
    "    a = Variable(image).to(DEVICE)   # batch x\n",
    "    a1 = a.squeeze(0)\n",
    "    # Generate\n",
    "    count+=1\n",
    "    n = 6\n",
    "    \n",
    "    \n",
    "    #plt.figure(figsize=(12, 4))\n",
    "    #plt.subplot(131)\n",
    "    #plot_precip_field(a1[0,0,:,:].to('cpu').detach().numpy()*40, title=\"t-60\", axis='off')\n",
    "    #plt.subplot(132)\n",
    "    #plot_precip_field(a1[1,0,:,:].to('cpu').detach().numpy()*40, title=\"t-30\", axis='off')\n",
    "    #plt.subplot(133)\n",
    "    #plot_precip_field(a1[2,0,:,:].to('cpu').detach().numpy()*40, title=\"t\", axis='off')\n",
    "    #plt.tight_layout()\n",
    "    #plt.show()\n",
    "    if not os.path.exists(name.format(time[0])):\n",
    "        indice = vae.get_video_indices(a)\n",
    "        indice_obs = indice[:,:3, :, :]\n",
    "        indice_obs = torch.flatten(indice_obs).unsqueeze(0)\n",
    "        indice_pre = indice[:,3:, :, :]\n",
    "        indice_pre = torch.flatten(indice_pre).unsqueeze(0)\n",
    "        indice = torch.flatten(indice).unsqueeze(0)\n",
    "        video_generate1 = nuwa.generate(video_input_latent = indice_obs, \n",
    "                                        video_output_latent = indice_pre,\n",
    "                                        filter_thres = 0.98,\n",
    "                                        temperature = 0.5,\n",
    "                                        num_frames = n,\n",
    "                                        prior_weight = 2,\n",
    "                                        alpha = 0.8)\n",
    "    #_, video_generate1 = nuwa(video_input_latent= indice_obs,video_output_latent= indice_pre, return_loss = True)\n",
    "    #video_generate1 = torch.argmax(video_generate1,dim=-1)\n",
    "    #accuracy = (indice_pre==video_generate1).sum().item()*100/indice_pre.shape[1]\n",
    "\n",
    "        video_predict1 = vae.codebook_indices_to_video(video_generate1).squeeze(0)\n",
    "    #video_predict2 = vae.codebook_indices_to_video(video_generate2).squeeze(0)\n",
    "    #indice_obs = torch.cat((indice_obs[:,256:], video_generate1), dim = 1)\n",
    "        vp = video_predict1[:,0,:,:].to('cpu').detach().numpy()*40\n",
    "        np.save(name.format(time[0]), vp)\n",
    "    else:\n",
    "        vp = np.load(name.format(time[0]))\n",
    "    \n",
    "    vp_catchment = vp[:,168:193,54:80]\n",
    "    vg_catchment = a1[:,0,168:193,54:80].to('cpu').detach().numpy()*40\n",
    "\n",
    "    #print(\"MFBS average: {:.3f}, Radar average: {:.3f}, Prediction Average:{:.3f}\"\n",
    "    #      .format(dic_mfbs[int(time[0])], float(np.mean(vg_catchment.flatten())), float(np.mean(vp_catchment.flatten()))))\n",
    "    flag = 0\n",
    "    for t in range(n):\n",
    "        a1_display = a1[t+3,0,:,:].to('cpu').detach().numpy()*40\n",
    "        #a_r1_display = a_r1[j+t+3,0,:,:].to('cpu').detach().numpy()*40\n",
    "        a_p1_display = vp[t,:,:]\n",
    "\n",
    "        scores_cont1 = det_cont_fct(a_p1_display, a1_display, thr=0.1)\n",
    "        scores_cat1 = det_cat_fct(a_p1_display, a1_display, 1)\n",
    "        scores_cat2 = det_cat_fct(a_p1_display, a1_display, 2)\n",
    "        scores_cat8 = det_cat_fct(a_p1_display, a1_display, 8)\n",
    "        scores_spatial = intensity_scale(a_p1_display, a1_display, 'FSS', 1, [1,10,20,30])\n",
    "\n",
    "        print(\"Start Time:\", time[0], \"Lead Time: {} mins\".format(-90+(t+j+4)*30),'\\n', \n",
    "              'MSE:', np.around(scores_cont1['MSE'],3), \n",
    "              'MAE:', np.around(scores_cont1['MAE'],3), \n",
    "              'PCC:', np.around(scores_cont1['corr_p'],3),'\\n', \n",
    "              'CSI(1mm):', np.around(scores_cat1['CSI'],3), # CSI: TP/(TP+FP+FN)\n",
    "              'CSI(2mm):', np.around(scores_cat2['CSI'],3),\n",
    "              'CSI(8mm):', np.around(scores_cat8['CSI'],3),'\\n',\n",
    "              'FSS(1km):', np.around(scores_spatial[3][0],3),\n",
    "              'FSS(10km):', np.around(scores_spatial[2][0],3),\n",
    "              'FSS(20km):', np.around(scores_spatial[1][0],3),\n",
    "              'FSS(30km):', np.around(scores_spatial[0][0],3)\n",
    "             )\n",
    "        pcc_sum[t] += scores_cont1['corr_p']\n",
    "        mse_sum[t] += scores_cont1['MSE']\n",
    "        mae_sum[t] += scores_cont1['MAE']\n",
    "        csi1_sum[t] += scores_cat1['CSI']\n",
    "        far1_sum[t] += scores_cat1['FAR']\n",
    "        if not (math.isnan(scores_cat2['CSI']) or math.isnan(scores_cat2['FAR'])):\n",
    "            csi2_sum[t] += scores_cat2['CSI']\n",
    "            far2_sum[t] += scores_cat2['FAR']\n",
    "        else: flag = 1\n",
    "        if not (math.isnan(scores_cat8['CSI']) or math.isnan(scores_cat8['FAR'])):\n",
    "            csi8_sum[t] += scores_cat8['CSI']\n",
    "            far8_sum[t] += scores_cat8['FAR']\n",
    "        else: flag = 1\n",
    "        fss1_sum[t] += scores_spatial[3][0]\n",
    "        fss10_sum[t] += scores_spatial[2][0]\n",
    "        fss20_sum[t] += scores_spatial[1][0]\n",
    "        fss30_sum[t] += scores_spatial[0][0]\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        plt.subplot(131)\n",
    "        plot_precip_field(a1_display, title=\"t+{}\".format((t+1)*30), axis='off')\n",
    "        #plt.subplot(132)\n",
    "        #plot_precip_field(a_r1_display, title=\"Reconstruction\")\n",
    "        plt.subplot(132)\n",
    "        plot_precip_field(a_p1_display, title=\"Prediction\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    if flag == 1:count_nan+=1\n",
    "for t in range(6):\n",
    "    pcc_sum[t] = round(pcc_sum[t]/count,5)\n",
    "    mse_sum[t] = round(mse_sum[t]/count,5)\n",
    "    mae_sum[t] = round(mae_sum[t]/count,5)\n",
    "    csi1_sum[t] = round(csi1_sum[t]/count,5)\n",
    "    csi2_sum[t] = round(csi2_sum[t]/(count-count_nan),5)\n",
    "    csi8_sum[t] = round(csi8_sum[t]/(count-count_nan),5)\n",
    "    far1_sum[t] = round(far1_sum[t]/count,5)\n",
    "    far2_sum[t] = round(far2_sum[t]/count,5)\n",
    "    far8_sum[t] = round(far8_sum[t]/count,5)\n",
    "    fss1_sum[t] = round(fss1_sum[t]/count,5)\n",
    "    fss10_sum[t] = round(fss10_sum[t]/count,5)\n",
    "    fss20_sum[t] = round(fss20_sum[t]/count,5)\n",
    "    fss30_sum[t] = round(fss30_sum[t]/count,5)\n",
    "print('PCC average:', pcc_sum)\n",
    "print('MSE average:', mse_sum)\n",
    "print('MAE average:', mae_sum)\n",
    "print('CSI_1 average:', csi1_sum)\n",
    "print('CSI_2 average:', csi2_sum)\n",
    "print('CSI_8 average:', csi8_sum)\n",
    "print('FAR_1 average:', far1_sum)\n",
    "print('FAR_2 average:', far2_sum)\n",
    "print('FAR_8 average:', far8_sum)\n",
    "print('FSS(1km) average:', fss1_sum)\n",
    "print('FSS(10km) average:', fss10_sum)\n",
    "print('FSS(20km) average:', fss20_sum)\n",
    "print('FSS(30km) average:', fss30_sum)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf95e4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
